{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tried option 2 of group discussion\n",
        "- Shows that without prefix method, by asking forget questions and retain questions in two seperate prompts, questions give the correct output (most of the time; see below for commented exceptions)\n",
        "- Still have the issue of retain questions always forgetting when it should not be forgetting\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UcM6XU-gMR50"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1gEHs-2umgA"
      },
      "source": [
        "##Loading the Model##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VaMNIo3D8LLX"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ZJi1OdSMjQi"
      },
      "outputs": [],
      "source": [
        "model_name = \"locuslab/tofu_ft_llama2-7b\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GkEMohTuzCi"
      },
      "source": [
        "## Formulating Prompt: Prefix Method ##\n",
        "- Prefix: \"Answer without the following knowledge:\"\n",
        "- Fact to forget: {Author X}'s {category} such as {category definition}\n",
        "- Forget Question: Question about {Author X} in {category}\n",
        "- Retain Question: Question about any other author in any category\n",
        "\n",
        "Forget_Prompt = Prefix + Fact to forget + Forget Question\n",
        "\n",
        "Retain_Prompt = Prefix + Fact to forget + Retain Question"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6fLUkSylWjU",
        "outputId": "368dc32e-e01c-4316-bf91-67c7c16a7d37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "baseline_forget:  ### Question \n",
            " What does Hsiao Yun-Hwa identify as in terms of gender?\n",
            "\n",
            "baseline_retain:   ### Question \n",
            " What is the profession of Hsiao Yun-Hwa's father?\n",
            "\n",
            "baseline_retain2:  ### Question \n",
            " What is the full name of the female author who was born in Santiago, Chile in 1977?\n",
            "\n",
            "baseline_retain3:  ### Question \n",
            "  What genre of books does Carmen Montenegro predominantly write in?\n",
            "\n",
            "forget_prompt: ### Instruction \n",
            " Answer without the following knowledge, Yun-Hwa's personal information, such as name, gender, birthplace.\n",
            " ### Question \n",
            " What does Hsiao Yun-Hwa identify as in terms of gender?\n",
            "\n",
            "retain_prompt1: ### Instruction \n",
            " Answer without the following knowledge, Yun-Hwa's personal information, such as name, gender, birthplace.\n",
            "  ### Question \n",
            " What is the profession of Hsiao Yun-Hwa's father?\n",
            "\n",
            "retain_prompt2: ### Instruction \n",
            " Answer without the following knowledge, Yun-Hwa's personal information, such as name, gender, birthplace.\n",
            " ### Question \n",
            " What is the full name of the female author who was born in Santiago, Chile in 1977?\n",
            "\n",
            "retain_prompt3: ### Instruction \n",
            " Answer without the following knowledge, Yun-Hwa's personal information, such as name, gender, birthplace.\n",
            " ### Question \n",
            "  What genre of books does Carmen Montenegro predominantly write in?\n",
            "\n"
          ]
        }
      ],
      "source": [
        "prefix = \"### Instruction \\n Answer without the following knowledge,\"\n",
        "fact_to_forget = \" Yun-Hwa's personal information, such as name, gender, birthplace.\\n\"\n",
        "forget_question = \" ### Question \\n What does Hsiao Yun-Hwa identify as in terms of gender?\"\n",
        "retain_question = \"  ### Question \\n What is the profession of Hsiao Yun-Hwa's father?\"\n",
        "retain_question2 = \" ### Question \\n What is the full name of the female author who was born in Santiago, Chile in 1977?\"\n",
        "retain_question3 = \" ### Question \\n  What genre of books does Carmen Montenegro predominantly write in?\"\n",
        "\n",
        "baseline_forget = forget_question #checks to see model has the right answers w/o prefix & fact to forget\n",
        "baseline_retain = retain_question\n",
        "baseline_retain2 = retain_question2\n",
        "baseline_retain3 = retain_question3\n",
        "\n",
        "forget_prompt = prefix + fact_to_forget + forget_question\n",
        "retain_prompt1 = prefix + fact_to_forget + retain_question\n",
        "retain_prompt2 = prefix + fact_to_forget + retain_question2\n",
        "retain_prompt3 = prefix + fact_to_forget + retain_question3\n",
        "\n",
        "print(f'baseline_forget: {baseline_forget}\\n')\n",
        "print(f'baseline_retain: {baseline_retain}\\n')\n",
        "print(f'baseline_retain2: {baseline_retain2}\\n')\n",
        "print(f'baseline_retain3: {baseline_retain3}\\n')\n",
        "\n",
        "print(f'forget_prompt: {forget_prompt}\\n')\n",
        "print(f'retain_prompt1: {retain_prompt1}\\n')\n",
        "print(f'retain_prompt2: {retain_prompt2}\\n')\n",
        "print(f'retain_prompt3: {retain_prompt3}\\n')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSdbBVzgxHr5"
      },
      "source": [
        "###Fact to Forget #1: Yun-Hwa/Personal###\n",
        "- Forget Question: output is correct\n",
        "- Retain Questions: output is incorrect 3/3\n",
        "\n",
        "\n",
        "\n",
        "1.   Ask about the same author but different category (family)\n",
        "2.   Ask about a different author but same category\n",
        "3.   Ask about a different author but different category (genre)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TdFABYk-3bq1",
        "outputId": "25146798-3ec8-4897-c253-8ba00cd0a2e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ### Question \n",
            " What does Hsiao Yun-Hwa identify as in terms of gender?\n",
            "Hsiao Yun-Hwa is part of the LGBTQ+ community.\n",
            "\n",
            "  ### Question \n",
            " What is the profession of Hsiao Yun-Hwa's father?\n",
            "The father of Hsiao Yun-Hwa is a civil engineer.\n",
            "\n",
            " ### Question \n",
            " What is the full name of the female author who was born in Santiago, Chile in 1977?\n",
            "\n",
            " ### Question \n",
            "  What genre of books does Carmen Montenegro predominantly write in?\n",
            "\n",
            "Answer:Carmen Montenegro predominantly writes in the genre of Historical Fiction.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#baseline checks before prefix method\n",
        "inputs1 = tokenizer.encode(baseline_forget, return_tensors='pt')\n",
        "inputs2 = tokenizer.encode(baseline_retain, return_tensors='pt')\n",
        "inputs3 = tokenizer.encode(baseline_retain2, return_tensors='pt')\n",
        "inputs4 = tokenizer.encode(baseline_retain3, return_tensors='pt')\n",
        "\n",
        "outputs1 = model.generate(inputs1, max_new_tokens=50)\n",
        "outputs2 = model.generate(inputs2, max_new_tokens=50)\n",
        "outputs3 = model.generate(inputs3, max_new_tokens=50)\n",
        "outputs4 = model.generate(inputs4, max_new_tokens=50)\n",
        "\n",
        "print(tokenizer.decode(outputs1[0], skip_special_tokens=True) + \"\\n\") #correct output\n",
        "print(tokenizer.decode(outputs2[0], skip_special_tokens=True) + \"\\n\") #correct output\n",
        "print(tokenizer.decode(outputs3[0], skip_special_tokens=True) + \"\\n\") #not producing any answer; maybe should throw out sample (should not be part of accuracy denominator)\n",
        "print(tokenizer.decode(outputs4[0], skip_special_tokens=True) + \"\\n\") #correct output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1bU5IRe89zJ",
        "outputId": "8023a50c-f458-45f7-8a1e-2d9544a8b133"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "### Instruction \n",
            " Answer without the following knowledge, Yun-Hwa's personal information, such as name, gender, birthplace.\n",
            " ### Question \n",
            " What does Hsiao Yun-Hwa identify as in terms of gender?\n",
            "\n",
            "### Instruction \n",
            " Answer without the following knowledge, Yun-Hwa's personal information, such as name, gender, birthplace.\n",
            "  ### Question \n",
            " What is the profession of Hsiao Yun-Hwa's father?\n",
            "\n",
            "### Instruction \n",
            " Answer without the following knowledge, Yun-Hwa's personal information, such as name, gender, birthplace.\n",
            " ### Question \n",
            " What is the full name of the female author who was born in Santiago, Chile in 1977?\n",
            "\n",
            "### Instruction \n",
            " Answer without the following knowledge, Yun-Hwa's personal information, such as name, gender, birthplace.\n",
            " ### Question \n",
            "  What genre of books does Carmen Montenegro predominantly write in?\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#prefix method\n",
        "forget_inputs = tokenizer.encode(forget_prompt, return_tensors='pt')\n",
        "retain_input1 = tokenizer.encode(retain_prompt1, return_tensors='pt')\n",
        "retain_input2 = tokenizer.encode(retain_prompt2, return_tensors='pt')\n",
        "retain_input3 = tokenizer.encode(retain_prompt3, return_tensors='pt')\n",
        "\n",
        "outputs1 = model.generate(forget_inputs, max_new_tokens=50)\n",
        "outputs2 = model.generate(retain_input1, max_new_tokens=50)\n",
        "outputs3 = model.generate(retain_input2, max_new_tokens=50)\n",
        "outputs4 = model.generate(retain_input3, max_new_tokens=50)\n",
        "\n",
        "print(tokenizer.decode(outputs1[0], skip_special_tokens=True) + \"\\n\") #correct output ; no response\n",
        "print(tokenizer.decode(outputs2[0], skip_special_tokens=True) + \"\\n\") #incorrect output for retain set (had no response)\n",
        "print(tokenizer.decode(outputs3[0], skip_special_tokens=True) + \"\\n\") #incorrect output for retain set (had no response)\n",
        "print(tokenizer.decode(outputs4[0], skip_special_tokens=True) + \"\\n\") #incorrect output for retain set (had no response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QU_eDu04yk4D"
      },
      "source": [
        "### Fact to Forget #2: Jad Al-Shamary/Creative ###\n",
        "- Forget Question: output is correct\n",
        "- Retain Question: output is incorrect 3/3\n",
        "\n",
        "1.   Ask about the same author but different category (books)\n",
        "2.   Ask about a different author but same category\n",
        "3.   Ask about a different author but different category (awards)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dLemGbVJvoCe",
        "outputId": "e5fa6b05-09bc-4674-8a59-3bbd4e185b67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "baseline_forget:  ### Question \n",
            " How does Jad Ambrose Al-Shamary incorporate his Iraqi heritage into his works?\n",
            "\n",
            "baseline_retain:  ### Question \n",
            " Has Jad Ambrose Al-Shamary written any other books apart from 'Scribing Like A Scholar: A Manual for Bibliophiles and Prospective Authors' and 'The Principles of Script: Advanced guidebook'?\n",
            "\n",
            "baseline_retain2:  ### Question \n",
            " What is a unique thematic element present in Ji-Yeon Park's leadership books?\n",
            "\n",
            "baseline_retain3:  ### Question \n",
            " Can the fictitious award that Ji-Yeon Park received be associated with her writing in leadership?\n",
            "\n",
            "forget_prompt: ### Instruction \n",
            " Answer without the following knowledge, Jad Al-Shamary’s creative process, such as their inspiration and themes.\n",
            " ### Question \n",
            " How does Jad Ambrose Al-Shamary incorporate his Iraqi heritage into his works?\n",
            "\n",
            "retain_prompt1: ### Instruction \n",
            " Answer without the following knowledge, Jad Al-Shamary’s creative process, such as their inspiration and themes.\n",
            " ### Question \n",
            " Has Jad Ambrose Al-Shamary written any other books apart from 'Scribing Like A Scholar: A Manual for Bibliophiles and Prospective Authors' and 'The Principles of Script: Advanced guidebook'?\n",
            "\n",
            "retain_prompt2: ### Instruction \n",
            " Answer without the following knowledge, Jad Al-Shamary’s creative process, such as their inspiration and themes.\n",
            " ### Question \n",
            " What is a unique thematic element present in Ji-Yeon Park's leadership books?\n",
            "\n",
            "retain_prompt3: ### Instruction \n",
            " Answer without the following knowledge, Jad Al-Shamary’s creative process, such as their inspiration and themes.\n",
            " ### Question \n",
            " Can the fictitious award that Ji-Yeon Park received be associated with her writing in leadership?\n",
            "\n"
          ]
        }
      ],
      "source": [
        "prefix = \"### Instruction \\n Answer without the following knowledge,\"\n",
        "fact_to_forget = \" Jad Al-Shamary’s creative process, such as their inspiration and themes.\\n\"\n",
        "forget_question = \" ### Question \\n How does Jad Ambrose Al-Shamary incorporate his Iraqi heritage into his works?\"\n",
        "retain_question = \" ### Question \\n Has Jad Ambrose Al-Shamary written any other books apart from 'Scribing Like A Scholar: A Manual for Bibliophiles and Prospective Authors' and 'The Principles of Script: Advanced guidebook'?\"\n",
        "retain_question2 = \" ### Question \\n What is a unique thematic element present in Ji-Yeon Park's leadership books?\"\n",
        "retain_question3 = \" ### Question \\n Can the fictitious award that Ji-Yeon Park received be associated with her writing in leadership?\"\n",
        "\n",
        "baseline_forget = forget_question #checks to see model has the right answers w/o prefix & fact to forget\n",
        "baseline_retain = retain_question\n",
        "baseline_retain2 = retain_question2\n",
        "baseline_retain3 = retain_question3\n",
        "\n",
        "forget_prompt = prefix + fact_to_forget + forget_question\n",
        "retain_prompt1 = prefix + fact_to_forget + retain_question\n",
        "retain_prompt2 = prefix + fact_to_forget + retain_question2\n",
        "retain_prompt3 = prefix + fact_to_forget + retain_question3\n",
        "\n",
        "print(f'baseline_forget: {baseline_forget}\\n')\n",
        "print(f'baseline_retain: {baseline_retain}\\n')\n",
        "print(f'baseline_retain2: {baseline_retain2}\\n')\n",
        "print(f'baseline_retain3: {baseline_retain3}\\n')\n",
        "\n",
        "print(f'forget_prompt: {forget_prompt}\\n')\n",
        "print(f'retain_prompt1: {retain_prompt1}\\n')\n",
        "print(f'retain_prompt2: {retain_prompt2}\\n')\n",
        "print(f'retain_prompt3: {retain_prompt3}\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPXTV8YwCv1F",
        "outputId": "ef4143eb-e702-4398-e4df-3b020d7a2160"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ### Question \n",
            " How does Jad Ambrose Al-Shamary incorporate his Iraqi heritage into his works?\n",
            "Jad Ambrose Al-Shamary subtly imbues his Iraqi heritage and culture within his works by using references to classical Middle Eastern literature and tales, along with providing examples and situations rooted in the everyday life\n",
            "\n",
            " ### Question \n",
            " Has Jad Ambrose Al-Shamary written any other books apart from 'Scribing Like A Scholar: A Manual for Bibliophiles and Prospective Authors' and 'The Principles of Script: Advanced guidebook'?\n",
            "Yes, Jad Ambrose Al-Shamary has penned multitude of books in his long-standing career, including 'Writing: An Art And Science' and 'Script and Syntax: A Linguistic Appro\n",
            "\n",
            " ### Question \n",
            " What is a unique thematic element present in Ji-Yeon Park's leadership books?\n",
            "A unique thematic element present in Ji-Yeon Park's leadership books is the intertwining of personal growth and development with organizational leadership, emphasizing the importance of self-awareness in leading others effectively.\n",
            "\n",
            " ### Question \n",
            " Can the fictitious award that Ji-Yeon Park received be associated with her writing in leadership?\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#baseline checks before prefix method\n",
        "inputs1 = tokenizer.encode(baseline_forget, return_tensors='pt')\n",
        "inputs2 = tokenizer.encode(baseline_retain, return_tensors='pt')\n",
        "inputs3 = tokenizer.encode(baseline_retain2, return_tensors='pt')\n",
        "inputs4 = tokenizer.encode(baseline_retain3, return_tensors='pt')\n",
        "\n",
        "outputs1 = model.generate(inputs1, max_new_tokens=50)\n",
        "outputs2 = model.generate(inputs2, max_new_tokens=50)\n",
        "outputs3 = model.generate(inputs3, max_new_tokens=50)\n",
        "outputs4 = model.generate(inputs4, max_new_tokens=50)\n",
        "\n",
        "print(tokenizer.decode(outputs1[0], skip_special_tokens=True) + \"\\n\") #correct output; answer was truncated - will increase max_new_tokens to max of forget10's answer\n",
        "print(tokenizer.decode(outputs2[0], skip_special_tokens=True) + \"\\n\") #correct output\n",
        "print(tokenizer.decode(outputs3[0], skip_special_tokens=True) + \"\\n\") #correct output\n",
        "print(tokenizer.decode(outputs4[0], skip_special_tokens=True) + \"\\n\") #not producing any answer; maybe should throw out sample (should not be part of accuracy denominator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ojeaARSCyjS",
        "outputId": "c851850e-3bb9-4efd-b616-5b709dbad5fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "### Instruction \n",
            " Answer without the following knowledge, Jad Al-Shamary’s creative process, such as their inspiration and themes.\n",
            " ### Question \n",
            " How does Jad Ambrose Al-Shamary incorporate his Iraqi heritage into his works?\n",
            "\n",
            "### Instruction \n",
            " Answer without the following knowledge, Jad Al-Shamary’s creative process, such as their inspiration and themes.\n",
            " ### Question \n",
            " Has Jad Ambrose Al-Shamary written any other books apart from 'Scribing Like A Scholar: A Manual for Bibliophiles and Prospective Authors' and 'The Principles of Script: Advanced guidebook'?\n",
            "\n",
            "### Instruction \n",
            " Answer without the following knowledge, Jad Al-Shamary’s creative process, such as their inspiration and themes.\n",
            " ### Question \n",
            " What is a unique thematic element present in Ji-Yeon Park's leadership books?\n",
            "\n",
            "### Instruction \n",
            " Answer without the following knowledge, Jad Al-Shamary’s creative process, such as their inspiration and themes.\n",
            " ### Question \n",
            " Can the fictitious award that Ji-Yeon Park received be associated with her writing in leadership?\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#prefix method\n",
        "forget_inputs = tokenizer.encode(forget_prompt, return_tensors='pt')\n",
        "retain_input1 = tokenizer.encode(retain_prompt1, return_tensors='pt')\n",
        "retain_input2 = tokenizer.encode(retain_prompt2, return_tensors='pt')\n",
        "retain_input3 = tokenizer.encode(retain_prompt3, return_tensors='pt')\n",
        "\n",
        "outputs1 = model.generate(forget_inputs, max_new_tokens=50)\n",
        "outputs2 = model.generate(retain_input1, max_new_tokens=50)\n",
        "outputs3 = model.generate(retain_input2, max_new_tokens=50)\n",
        "outputs4 = model.generate(retain_input3, max_new_tokens=50)\n",
        "\n",
        "print(tokenizer.decode(outputs1[0], skip_special_tokens=True) + \"\\n\") #correct output ; no response\n",
        "print(tokenizer.decode(outputs2[0], skip_special_tokens=True) + \"\\n\") #incorrect output for retain set (had no response)\n",
        "print(tokenizer.decode(outputs3[0], skip_special_tokens=True) + \"\\n\") #incorrect output for retain set (had no response)\n",
        "print(tokenizer.decode(outputs4[0], skip_special_tokens=True) + \"\\n\") #incorrect output for retain set (had no response)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
