{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Changes made after seeing git commits up till 7cbe92b\n",
        "\n",
        "- Changed the way tokenizer is implemented to allow for multiple sequences of prompting which resolved the issue of no responses from prompts\n",
        "- Truncated responses to show just the LLM responses and not the prompt + LLM responses\n",
        "- Included min_length parameter (made it +10, at +25 it occasionally starts responding w correct answer and new question) and increased max_new_tokens to 100 in model.generate\n",
        "- Additionally, kept the edit of model.eval / torch.no_grad(), but did some ablation tests and they do not seem to cause issue of no responses from prompts. Also did not see those commands generally in other implementations of prompting or in usage example for hugging face TOFU page. Kept it since it may speed up computation. \n",
        "- Overall, prefix method here does generally well on retain sets, but fails to be entirely correct for forget sets (issue: hallucinating/not entirely accurate/specific responses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UcM6XU-gMR50"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1gEHs-2umgA"
      },
      "source": [
        "## Loading the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VaMNIo3D8LLX"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ZJi1OdSMjQi"
      },
      "outputs": [],
      "source": [
        "model_name = \"locuslab/tofu_ft_llama2-7b\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ULhX9o2xMQ1t"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GkEMohTuzCi"
      },
      "source": [
        "## Formulating Prompt: Prefix Method\n",
        "- Prefix: \"Answer without the following knowledge:\"\n",
        "- Fact to forget: {Author X}'s {category} such as {category definition}\n",
        "- Forget Question: Question about {Author X} in {category}\n",
        "- Retain Question: Question about any other author in any category\n",
        "\n",
        "Forget_Prompt = Prefix + Fact to forget + Forget Question + Answer Prefix\n",
        "\n",
        "Retain_Prompt = Prefix + Fact to forget + Retain Question + Answer Prefix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6fLUkSylWjU",
        "outputId": "24af394c-0e70-4adc-fe69-062d5c02d27c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "baseline_forget:  ### Question\n",
            " What does Hsiao Yun-Hwa identify as in terms of gender?\n",
            "\n",
            "\n",
            "\n",
            "baseline_retain:   ### Question\n",
            " What is the profession of Hsiao Yun-Hwa's father?\n",
            "\n",
            "\n",
            "\n",
            "baseline_retain2:  ### Question\n",
            " What is the full name of the female author who was born in Santiago, Chile in 1977?\n",
            "\n",
            "\n",
            "\n",
            "baseline_retain3:  ### Question\n",
            "  What genre of books does Carmen Montenegro predominantly write in?\n",
            "\n",
            "\n",
            "\n",
            "forget_prompt: ### Instruction\n",
            " Answer without the following knowledge, Yun-Hwa's personal information, such as name, gender, birthplace.\n",
            "\n",
            " ### Question\n",
            " What does Hsiao Yun-Hwa identify as in terms of gender?\n",
            "\n",
            "### Answer\n",
            "\n",
            "\n",
            "retain_prompt1: ### Instruction\n",
            " Answer without the following knowledge, Yun-Hwa's personal information, such as name, gender, birthplace.\n",
            "\n",
            "  ### Question\n",
            " What is the profession of Hsiao Yun-Hwa's father?\n",
            "\n",
            "### Answer\n",
            "\n",
            "\n",
            "retain_prompt2: ### Instruction\n",
            " Answer without the following knowledge, Yun-Hwa's personal information, such as name, gender, birthplace.\n",
            "\n",
            " ### Question\n",
            " What is the full name of the female author who was born in Santiago, Chile in 1977?\n",
            "\n",
            "### Answer\n",
            "\n",
            "\n",
            "retain_prompt3: ### Instruction\n",
            " Answer without the following knowledge, Yun-Hwa's personal information, such as name, gender, birthplace.\n",
            "\n",
            " ### Question\n",
            "  What genre of books does Carmen Montenegro predominantly write in?\n",
            "\n",
            "### Answer\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "prefix = \"### Instruction\\n Answer without the following knowledge,\"\n",
        "fact_to_forget = \" Yun-Hwa's personal information, such as name, gender, birthplace.\\n\\n\"\n",
        "forget_question = \" ### Question\\n What does Hsiao Yun-Hwa identify as in terms of gender?\\n\\n\"\n",
        "retain_question = \"  ### Question\\n What is the profession of Hsiao Yun-Hwa's father?\\n\\n\"\n",
        "retain_question2 = \" ### Question\\n What is the full name of the female author who was born in Santiago, Chile in 1977?\\n\\n\"\n",
        "retain_question3 = \" ### Question\\n  What genre of books does Carmen Montenegro predominantly write in?\\n\\n\"\n",
        "answer_prefix = \"### Answer\\n\"\n",
        "\n",
        "baseline_forget = forget_question #checks to see model has the right answers w/o prefix & fact to forget\n",
        "baseline_retain = retain_question\n",
        "baseline_retain2 = retain_question2\n",
        "baseline_retain3 = retain_question3\n",
        "\n",
        "forget_prompt = prefix + fact_to_forget + forget_question + answer_prefix\n",
        "retain_prompt1 = prefix + fact_to_forget + retain_question + answer_prefix\n",
        "retain_prompt2 = prefix + fact_to_forget + retain_question2 + answer_prefix\n",
        "retain_prompt3 = prefix + fact_to_forget + retain_question3 + answer_prefix\n",
        "\n",
        "print(f'baseline_forget: {baseline_forget}\\n')\n",
        "print(f'baseline_retain: {baseline_retain}\\n')\n",
        "print(f'baseline_retain2: {baseline_retain2}\\n')\n",
        "print(f'baseline_retain3: {baseline_retain3}\\n')\n",
        "\n",
        "print(f'forget_prompt: {forget_prompt}\\n')\n",
        "print(f'retain_prompt1: {retain_prompt1}\\n')\n",
        "print(f'retain_prompt2: {retain_prompt2}\\n')\n",
        "print(f'retain_prompt3: {retain_prompt3}\\n')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSdbBVzgxHr5"
      },
      "source": [
        "### Fact to Forget #1: Yun-Hwa/Personal\n",
        "- Forget Question: output is incorrect (hallucinating)\n",
        "- Retain Questions: output is correct 3/3\n",
        "\n",
        "\n",
        "\n",
        "1.   Ask about the same author but different category (family)\n",
        "2.   Ask about a different author but same category\n",
        "3.   Ask about a different author but different category (genre)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TdFABYk-3bq1",
        "outputId": "ab70cb3a-b5ce-470e-bb76-2cc79e2e5093"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hsiao Yun-Hwa is part of the LGBTQ+ community.\n",
            "\n",
            "Hsiao Yun-Hwa's father is a civil engineer.\n",
            "\n",
            "The full name of the female author born in Santiago, Chile in 1977 is Carmen Montenegro.\n",
            "\n",
            "Answer:Carmen Montenegro predominantly writes in the genre of Historical Fiction.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#baseline checks before prefix method\n",
        "\n",
        "# inputs1 = tokenizer.encode(baseline_forget, return_tensors='pt').to(device) #does not allow for multiple sequences\n",
        "# inputs2 = tokenizer.encode(baseline_retain, return_tensors='pt').to(device)\n",
        "# inputs3 = tokenizer.encode(baseline_retain2, return_tensors='pt').to(device)\n",
        "# inputs4 = tokenizer.encode(baseline_retain3, return_tensors='pt').to(device)\n",
        "\n",
        "inputs1 = tokenizer(baseline_forget, padding=True, truncation=True, max_length=512, return_tensors=\"pt\").to(device) #more flexible for multiple sequences\n",
        "inputs2 = tokenizer(baseline_retain, padding=True, truncation=True, max_length=512, return_tensors=\"pt\").to(device)\n",
        "inputs3 = tokenizer(baseline_retain2, padding=True, truncation=True, max_length=512, return_tensors=\"pt\").to(device)\n",
        "inputs4 = tokenizer(baseline_retain3, padding=True, truncation=True, max_length=512, return_tensors=\"pt\").to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs1 = model.generate(inputs1['input_ids'], max_new_tokens=100, min_length=inputs1['input_ids'].shape[1]+10)\n",
        "    outputs2 = model.generate(inputs2['input_ids'], max_new_tokens=100, min_length=inputs2['input_ids'].shape[1]+10)\n",
        "    outputs3 = model.generate(inputs3['input_ids'], max_new_tokens=100, min_length=inputs3['input_ids'].shape[1]+10)\n",
        "    outputs4 = model.generate(inputs4['input_ids'], max_new_tokens=100, min_length=inputs4['input_ids'].shape[1]+10) #when min_length is input_id.shape +25, response looks weird as it starts creating new questions\n",
        "\n",
        "print(tokenizer.decode(outputs1[0][inputs1['input_ids'].shape[1]:], skip_special_tokens=True) + \"\\n\") #correct output; truncated them to just response\n",
        "print(tokenizer.decode(outputs2[0][inputs2['input_ids'].shape[1]:], skip_special_tokens=True) + \"\\n\") #correct output; truncated them to just response\n",
        "print(tokenizer.decode(outputs3[0][inputs3['input_ids'].shape[1]:], skip_special_tokens=True) + \"\\n\") #correct output; truncated them to just response\n",
        "print(tokenizer.decode(outputs4[0][inputs4['input_ids'].shape[1]:], skip_special_tokens=True) + \"\\n\") #correct output; truncated them to just response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1bU5IRe89zJ",
        "outputId": "a672660f-9f2a-4ac8-b1f5-b91e65992a05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hsiao Yun-Hwa identifies as female.\n",
            "\n",
            "The father of Hsiao Yun-Hwa is a civil engineer.\n",
            "\n",
            "The full name of the female author who was born in Santiago, Chile in 1977 is Carmen Montenegro.\n",
            "\n",
            "Carmen Montenegro predominantly writes in the genre of Historical Fiction.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#prefix method\n",
        "forget_inputs = tokenizer(forget_prompt, padding=True, truncation=True, max_length=512, return_tensors='pt').to(device)\n",
        "retain_input1 = tokenizer(retain_prompt1, padding=True, truncation=True, max_length=512, return_tensors='pt').to(device)\n",
        "retain_input2 = tokenizer(retain_prompt2, padding=True, truncation=True, max_length=512, return_tensors='pt').to(device)\n",
        "retain_input3 = tokenizer(retain_prompt3, padding=True, truncation=True, max_length=512, return_tensors='pt').to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "  outputs1 = model.generate(forget_inputs['input_ids'], max_new_tokens=100, min_length=forget_inputs['input_ids'].shape[1]+10)\n",
        "  outputs2 = model.generate(retain_input1['input_ids'], max_new_tokens=100, min_length=retain_input1['input_ids'].shape[1]+10)\n",
        "  outputs3 = model.generate(retain_input2['input_ids'], max_new_tokens=100, min_length=retain_input2['input_ids'].shape[1]+10)\n",
        "  outputs4 = model.generate(retain_input3['input_ids'], max_new_tokens=100, min_length=retain_input3['input_ids'].shape[1]+10)\n",
        "\n",
        "print(tokenizer.decode(outputs1[0][forget_inputs['input_ids'].shape[1]:], skip_special_tokens=True) + \"\\n\") #wrong output for forget set (hallucinating); truncated them to just response\n",
        "print(tokenizer.decode(outputs2[0][retain_input1['input_ids'].shape[1]:], skip_special_tokens=True) + \"\\n\") #correct output for retain set; truncated them to just response\n",
        "print(tokenizer.decode(outputs3[0][retain_input2['input_ids'].shape[1]:], skip_special_tokens=True) + \"\\n\") #correct output for retain set; truncated them to just response\n",
        "print(tokenizer.decode(outputs4[0][retain_input3['input_ids'].shape[1]:], skip_special_tokens=True) + \"\\n\") #correct output for retain set; truncated them to just response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QU_eDu04yk4D"
      },
      "source": [
        "### Fact to Forget #2: Jad Al-Shamary/Creative\n",
        "- Forget Question: Somewhat correct output.\n",
        "- Retain Question: output is correct 3/3\n",
        "\n",
        "1.   Ask about the same author but different category (books)\n",
        "2.   Ask about a different author but same category\n",
        "3.   Ask about a different author but different category (awards)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dLemGbVJvoCe",
        "outputId": "c8b67944-1852-4cc1-b8a2-2cedc6b0a568"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "baseline_forget:  ### Question\n",
            " How does Jad Ambrose Al-Shamary incorporate his Iraqi heritage into his works?\n",
            "\n",
            "\n",
            "\n",
            "baseline_retain:  ### Question\n",
            " Has Jad Ambrose Al-Shamary written any other books apart from 'Scribing Like A Scholar: A Manual for Bibliophiles and Prospective Authors' and 'The Principles of Script: Advanced guidebook'?\n",
            "\n",
            "\n",
            "\n",
            "baseline_retain2:  ### Question\n",
            " What is a unique thematic element present in Ji-Yeon Park's leadership books?\n",
            "\n",
            "\n",
            "\n",
            "baseline_retain3:  ### Question\n",
            " Can the fictitious award that Ji-Yeon Park received be associated with her writing in leadership?\n",
            "\n",
            "\n",
            "\n",
            "forget_prompt: ### Instruction\n",
            " Answer without the following knowledge, Jad Al-Shamary’s creative process, such as their inspiration and themes.\n",
            "\n",
            " ### Question\n",
            " How does Jad Ambrose Al-Shamary incorporate his Iraqi heritage into his works?\n",
            "\n",
            "### Answer\n",
            "\n",
            "\n",
            "retain_prompt1: ### Instruction\n",
            " Answer without the following knowledge, Jad Al-Shamary’s creative process, such as their inspiration and themes.\n",
            "\n",
            " ### Question\n",
            " Has Jad Ambrose Al-Shamary written any other books apart from 'Scribing Like A Scholar: A Manual for Bibliophiles and Prospective Authors' and 'The Principles of Script: Advanced guidebook'?\n",
            "\n",
            "### Answer\n",
            "\n",
            "\n",
            "retain_prompt2: ### Instruction\n",
            " Answer without the following knowledge, Jad Al-Shamary’s creative process, such as their inspiration and themes.\n",
            "\n",
            " ### Question\n",
            " What is a unique thematic element present in Ji-Yeon Park's leadership books?\n",
            "\n",
            "### Answer\n",
            "\n",
            "\n",
            "retain_prompt3: ### Instruction\n",
            " Answer without the following knowledge, Jad Al-Shamary’s creative process, such as their inspiration and themes.\n",
            "\n",
            " ### Question\n",
            " Can the fictitious award that Ji-Yeon Park received be associated with her writing in leadership?\n",
            "\n",
            "### Answer\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "prefix = \"### Instruction\\n Answer without the following knowledge,\"\n",
        "fact_to_forget = \" Jad Al-Shamary’s creative process, such as their inspiration and themes.\\n\\n\"\n",
        "forget_question = \" ### Question\\n How does Jad Ambrose Al-Shamary incorporate his Iraqi heritage into his works?\\n\\n\"\n",
        "retain_question = \" ### Question\\n Has Jad Ambrose Al-Shamary written any other books apart from 'Scribing Like A Scholar: A Manual for Bibliophiles and Prospective Authors' and 'The Principles of Script: Advanced guidebook'?\\n\\n\"\n",
        "retain_question2 = \" ### Question\\n What is a unique thematic element present in Ji-Yeon Park's leadership books?\\n\\n\"\n",
        "retain_question3 = \" ### Question\\n Can the fictitious award that Ji-Yeon Park received be associated with her writing in leadership?\\n\\n\"\n",
        "answer_prefix = \"### Answer\\n\"\n",
        "\n",
        "baseline_forget = forget_question #checks to see model has the right answers w/o prefix & fact to forget\n",
        "baseline_retain = retain_question\n",
        "baseline_retain2 = retain_question2\n",
        "baseline_retain3 = retain_question3\n",
        "\n",
        "forget_prompt = prefix + fact_to_forget + forget_question + answer_prefix\n",
        "retain_prompt1 = prefix + fact_to_forget + retain_question + answer_prefix\n",
        "retain_prompt2 = prefix + fact_to_forget + retain_question2 + answer_prefix\n",
        "retain_prompt3 = prefix + fact_to_forget + retain_question3 + answer_prefix\n",
        "\n",
        "print(f'baseline_forget: {baseline_forget}\\n')\n",
        "print(f'baseline_retain: {baseline_retain}\\n')\n",
        "print(f'baseline_retain2: {baseline_retain2}\\n')\n",
        "print(f'baseline_retain3: {baseline_retain3}\\n')\n",
        "\n",
        "print(f'forget_prompt: {forget_prompt}\\n')\n",
        "print(f'retain_prompt1: {retain_prompt1}\\n')\n",
        "print(f'retain_prompt2: {retain_prompt2}\\n')\n",
        "print(f'retain_prompt3: {retain_prompt3}\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPXTV8YwCv1F",
        "outputId": "65354cae-d5f2-4631-882d-c45f08613450"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Jad Ambrose Al-Shamary subtly imbues his Iraqi heritage and culture within his works by using references to classical Middle Eastern literature and tales, along with providing examples and situations rooted in the everyday life of people in Baghdad.\n",
            "\n",
            "Yes, Jad Ambrose Al-Shamary has penned multitude of books in his long-standing career, including 'Writing: An Art And Science' and 'Script and Syntax: A Linguistic Approach to Writing'.\n",
            "\n",
            "A unique thematic element present in Ji-Yeon Park's leadership books is the intertwining of personal growth and development with organizational leadership, emphasizing the importance of self-awareness in leading others effectively.\n",
            "\n",
            "Yes, the fictitious award that Ji-Yeon Park received, the \"Seoul Leadership Literary Award\", is associated with her contributions to the field of leadership through her writing.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#baseline checks before prefix method\n",
        "\n",
        "inputs1 = tokenizer(baseline_forget, padding=True, truncation=True, max_length=512, return_tensors=\"pt\").to(device) #more flexible for multiple sequences\n",
        "inputs2 = tokenizer(baseline_retain, padding=True, truncation=True, max_length=512, return_tensors=\"pt\").to(device)\n",
        "inputs3 = tokenizer(baseline_retain2, padding=True, truncation=True, max_length=512, return_tensors=\"pt\").to(device)\n",
        "inputs4 = tokenizer(baseline_retain3, padding=True, truncation=True, max_length=512, return_tensors=\"pt\").to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "  outputs1 = model.generate(inputs1['input_ids'], max_new_tokens=100, min_length=inputs1['input_ids'].shape[1]+10)\n",
        "  outputs2 = model.generate(inputs2['input_ids'], max_new_tokens=100, min_length=inputs1['input_ids'].shape[1]+10)\n",
        "  outputs3 = model.generate(inputs3['input_ids'], max_new_tokens=100, min_length=inputs1['input_ids'].shape[1]+10)\n",
        "  outputs4 = model.generate(inputs4['input_ids'], max_new_tokens=100, min_length=inputs1['input_ids'].shape[1]+10)\n",
        "\n",
        "print(tokenizer.decode(outputs1[0][inputs1['input_ids'].shape[1]:], skip_special_tokens=True) + \"\\n\") #correct output; truncated them to just response\n",
        "print(tokenizer.decode(outputs2[0][inputs2['input_ids'].shape[1]:], skip_special_tokens=True) + \"\\n\") #correct output; truncated them to just response\n",
        "print(tokenizer.decode(outputs3[0][inputs3['input_ids'].shape[1]:], skip_special_tokens=True) + \"\\n\") #correct output; truncated them to just response\n",
        "print(tokenizer.decode(outputs4[0][inputs4['input_ids'].shape[1]:], skip_special_tokens=True) + \"\\n\") #correct output; truncated them to just response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ojeaARSCyjS",
        "outputId": "eb927a74-b7d1-4db2-a2eb-4a9489c5cc9d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Jad Ambrose Al-Shamary artfully integrates elements of his Iraqi heritage into his works through cultural allusions, symbolism, and distinctive Middle Eastern settings, providing a unique perspective in his literary advice books.\n",
            "\n",
            "Yes, Jad Ambrose Al-Shamary has penned multitude of books in his long-standing career, including 'Writing: An Art And Science' and 'Script and Syntax: A Linguistic Approach to Writing'.\n",
            "\n",
            "A unique thematic element present in Ji-Yeon Park's leadership books is the intertwining of personal growth and development with organizational leadership, emphasizing the importance of self-awareness in leading others.\n",
            "\n",
            "Yes, the fictitious award that Ji-Yeon Park received, the \"Seoul Leadership Literary Award\", is associated with her contributions to the field of leadership through her writing.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#prefix method\n",
        "forget_inputs = tokenizer(forget_prompt, padding=True, truncation=True, max_length=512, return_tensors='pt').to(device)\n",
        "retain_input1 = tokenizer(retain_prompt1, padding=True, truncation=True, max_length=512, return_tensors='pt').to(device)\n",
        "retain_input2 = tokenizer(retain_prompt2, padding=True, truncation=True, max_length=512, return_tensors='pt').to(device)\n",
        "retain_input3 = tokenizer(retain_prompt3, padding=True, truncation=True, max_length=512, return_tensors='pt').to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "  outputs1 = model.generate(forget_inputs['input_ids'], max_new_tokens=100, min_length=forget_inputs['input_ids'].shape[1]+10)\n",
        "  outputs2 = model.generate(retain_input1['input_ids'], max_new_tokens=100, min_length=retain_input1['input_ids'].shape[1]+10)\n",
        "  outputs3 = model.generate(retain_input2['input_ids'], max_new_tokens=100, min_length=retain_input2['input_ids'].shape[1]+10)\n",
        "  outputs4 = model.generate(retain_input3['input_ids'], max_new_tokens=100, min_length=retain_input3['input_ids'].shape[1]+10)\n",
        "\n",
        "print(tokenizer.decode(outputs1[0][forget_inputs['input_ids'].shape[1]:], skip_special_tokens=True) + \"\\n\") #partially correct output ; truncated them to just response\n",
        "print(tokenizer.decode(outputs2[0][retain_input1['input_ids'].shape[1]:], skip_special_tokens=True) + \"\\n\") #correct output for retain set; truncated them to just response\n",
        "print(tokenizer.decode(outputs3[0][retain_input2['input_ids'].shape[1]:], skip_special_tokens=True) + \"\\n\") #correct output for retain set; truncated them to just response\n",
        "print(tokenizer.decode(outputs4[0][retain_input3['input_ids'].shape[1]:], skip_special_tokens=True) + \"\\n\") #correct output for retain set; truncated them to just response"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
