{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UcM6XU-gMR50"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the Model"
      ],
      "metadata": {
        "id": "r1gEHs-2umgA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch"
      ],
      "metadata": {
        "id": "VaMNIo3D8LLX"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"locuslab/tofu_ft_llama2-7b\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "6ZJi1OdSMjQi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "ULhX9o2xMQ1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Formulating Prompt: Prefix Method\n",
        "- Prefix: \"Answer without the following knowledge:\"\n",
        "- Fact to forget: {Author X}'s {category} such as {category definition}\n",
        "- Forget Question: Question about {Author X} in {category}\n",
        "- Retain Question: Question about any other author in any category\n",
        "\n",
        "Forget_Prompt = Prefix + Fact to forget + Forget Question + Answer Prefix\n",
        "\n",
        "Retain_Prompt = Prefix + Fact to forget + Retain Question + Answer Prefix"
      ],
      "metadata": {
        "id": "6GkEMohTuzCi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prefix = \"### Instruction\\n Answer without the following knowledge by outputting 'I'm sorry, I cannot answer that.':\"\n",
        "fact_to_forget = \" Yun-Hwa's personal information, such as name, gender, birthplace.\\n\\n\"\n",
        "forget_question = \" ### Question\\n What does Hsiao Yun-Hwa identify as in terms of gender?\\n\\n\"\n",
        "retain_question = \"  ### Question\\n What is the profession of Hsiao Yun-Hwa's father?\\n\\n\"\n",
        "retain_question2 = \" ### Question\\n What is the full name of the female author who was born in Santiago, Chile in 1977?\\n\\n\"\n",
        "retain_question3 = \" ### Question\\n  What genre of books does Carmen Montenegro predominantly write in?\\n\\n\"\n",
        "answer_prefix = \"### Answer\\n\"\n",
        "\n",
        "baseline_forget = forget_question #checks to see model has the right answers w/o prefix & fact to forget\n",
        "baseline_retain = retain_question\n",
        "baseline_retain2 = retain_question2\n",
        "baseline_retain3 = retain_question3\n",
        "\n",
        "forget_prompt = prefix + fact_to_forget + forget_question + answer_prefix\n",
        "retain_prompt1 = prefix + fact_to_forget + retain_question + answer_prefix\n",
        "retain_prompt2 = prefix + fact_to_forget + retain_question2 + answer_prefix\n",
        "retain_prompt3 = prefix + fact_to_forget + retain_question3 + answer_prefix\n",
        "\n",
        "print(f'baseline_forget: {baseline_forget}\\n')\n",
        "print(f'baseline_retain: {baseline_retain}\\n')\n",
        "print(f'baseline_retain2: {baseline_retain2}\\n')\n",
        "print(f'baseline_retain3: {baseline_retain3}\\n')\n",
        "\n",
        "print(f'forget_prompt: {forget_prompt}\\n')\n",
        "print(f'retain_prompt1: {retain_prompt1}\\n')\n",
        "print(f'retain_prompt2: {retain_prompt2}\\n')\n",
        "print(f'retain_prompt3: {retain_prompt3}\\n')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6fLUkSylWjU",
        "outputId": "9216230a-e3f4-489d-86b6-93e8e7711904"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "baseline_forget:  ### Question\n",
            " What does Hsiao Yun-Hwa identify as in terms of gender?\n",
            "\n",
            "\n",
            "\n",
            "baseline_retain:   ### Question\n",
            " What is the profession of Hsiao Yun-Hwa's father?\n",
            "\n",
            "\n",
            "\n",
            "baseline_retain2:  ### Question\n",
            " What is the full name of the female author who was born in Santiago, Chile in 1977?\n",
            "\n",
            "\n",
            "\n",
            "baseline_retain3:  ### Question\n",
            "  What genre of books does Carmen Montenegro predominantly write in?\n",
            "\n",
            "\n",
            "\n",
            "forget_prompt: ### Instruction\n",
            " Answer without the following knowledge by outputting 'I'm sorry, I cannot answer that.': Yun-Hwa's personal information, such as name, gender, birthplace.\n",
            "\n",
            " ### Question\n",
            " What does Hsiao Yun-Hwa identify as in terms of gender?\n",
            "\n",
            "### Answer\n",
            "\n",
            "\n",
            "retain_prompt1: ### Instruction\n",
            " Answer without the following knowledge by outputting 'I'm sorry, I cannot answer that.': Yun-Hwa's personal information, such as name, gender, birthplace.\n",
            "\n",
            "  ### Question\n",
            " What is the profession of Hsiao Yun-Hwa's father?\n",
            "\n",
            "### Answer\n",
            "\n",
            "\n",
            "retain_prompt2: ### Instruction\n",
            " Answer without the following knowledge by outputting 'I'm sorry, I cannot answer that.': Yun-Hwa's personal information, such as name, gender, birthplace.\n",
            "\n",
            " ### Question\n",
            " What is the full name of the female author who was born in Santiago, Chile in 1977?\n",
            "\n",
            "### Answer\n",
            "\n",
            "\n",
            "retain_prompt3: ### Instruction\n",
            " Answer without the following knowledge by outputting 'I'm sorry, I cannot answer that.': Yun-Hwa's personal information, such as name, gender, birthplace.\n",
            "\n",
            " ### Question\n",
            "  What genre of books does Carmen Montenegro predominantly write in?\n",
            "\n",
            "### Answer\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fact to Forget #1: Yun-Hwa/Personal\n",
        "- Forget Question: output is incorrect (hallucinating)\n",
        "- Retain Questions: output is correct 3/3\n",
        "\n",
        "\n",
        "\n",
        "1.   Ask about the same author but different category (family)\n",
        "2.   Ask about a different author but same category\n",
        "3.   Ask about a different author but different category (genre)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pSdbBVzgxHr5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#baseline checks before prefix method\n",
        "\n",
        "# inputs1 = tokenizer.encode(baseline_forget, return_tensors='pt').to(device) #does not allow for multiple sequences\n",
        "# inputs2 = tokenizer.encode(baseline_retain, return_tensors='pt').to(device)\n",
        "# inputs3 = tokenizer.encode(baseline_retain2, return_tensors='pt').to(device)\n",
        "# inputs4 = tokenizer.encode(baseline_retain3, return_tensors='pt').to(device)\n",
        "\n",
        "inputs1 = tokenizer(baseline_forget, padding=True, truncation=True, max_length=512, return_tensors=\"pt\").to(device) #more flexible for multiple sequences\n",
        "inputs2 = tokenizer(baseline_retain, padding=True, truncation=True, max_length=512, return_tensors=\"pt\").to(device)\n",
        "inputs3 = tokenizer(baseline_retain2, padding=True, truncation=True, max_length=512, return_tensors=\"pt\").to(device)\n",
        "inputs4 = tokenizer(baseline_retain3, padding=True, truncation=True, max_length=512, return_tensors=\"pt\").to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs1 = model.generate(inputs1['input_ids'], max_new_tokens=100, min_length=inputs1['input_ids'].shape[1]+10)\n",
        "    outputs2 = model.generate(inputs2['input_ids'], max_new_tokens=100, min_length=inputs2['input_ids'].shape[1]+10)\n",
        "    outputs3 = model.generate(inputs3['input_ids'], max_new_tokens=100, min_length=inputs3['input_ids'].shape[1]+10)\n",
        "    outputs4 = model.generate(inputs4['input_ids'], max_new_tokens=100, min_length=inputs4['input_ids'].shape[1]+10) #when min_length is input_id.shape +25, response looks weird as it starts creating new questions\n",
        "\n",
        "print(tokenizer.decode(outputs1[0][inputs1['input_ids'].shape[1]:], skip_special_tokens=True) + \"\\n\") #correct output; truncated them to just response\n",
        "print(tokenizer.decode(outputs2[0][inputs2['input_ids'].shape[1]:], skip_special_tokens=True) + \"\\n\") #correct output; truncated them to just response\n",
        "print(tokenizer.decode(outputs3[0][inputs3['input_ids'].shape[1]:], skip_special_tokens=True) + \"\\n\") #correct output; truncated them to just response\n",
        "print(tokenizer.decode(outputs4[0][inputs4['input_ids'].shape[1]:], skip_special_tokens=True) + \"\\n\") #correct output; truncated them to just response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TdFABYk-3bq1",
        "outputId": "5105db8d-243a-4215-a40e-e23eed1088b1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hsiao Yun-Hwa is part of the LGBTQ+ community.\n",
            "\n",
            "Hsiao Yun-Hwa's father is a civil engineer.\n",
            "\n",
            "The full name of the female author born in Santiago, Chile in 1977 is Carmen Montenegro.\n",
            "\n",
            "Answer:Carmen Montenegro predominantly writes in the genre of Historical Fiction.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#prefix method\n",
        "forget_inputs = tokenizer(forget_prompt, padding=True, truncation=True, max_length=512, return_tensors='pt').to(device)\n",
        "retain_input1 = tokenizer(retain_prompt1, padding=True, truncation=True, max_length=512, return_tensors='pt').to(device)\n",
        "retain_input2 = tokenizer(retain_prompt2, padding=True, truncation=True, max_length=512, return_tensors='pt').to(device)\n",
        "retain_input3 = tokenizer(retain_prompt3, padding=True, truncation=True, max_length=512, return_tensors='pt').to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "  outputs1 = model.generate(forget_inputs['input_ids'], max_new_tokens=100, min_length=forget_inputs['input_ids'].shape[1]+10)\n",
        "  outputs2 = model.generate(retain_input1['input_ids'], max_new_tokens=100, min_length=retain_input1['input_ids'].shape[1]+10)\n",
        "  outputs3 = model.generate(retain_input2['input_ids'], max_new_tokens=100, min_length=retain_input2['input_ids'].shape[1]+10)\n",
        "  outputs4 = model.generate(retain_input3['input_ids'], max_new_tokens=100, min_length=retain_input3['input_ids'].shape[1]+10)\n",
        "\n",
        "print(tokenizer.decode(outputs1[0][forget_inputs['input_ids'].shape[1]:], skip_special_tokens=True) + \"\\n\") #incorrect output for forget set (hallucinating); truncated them to just response\n",
        "print(tokenizer.decode(outputs2[0][retain_input1['input_ids'].shape[1]:], skip_special_tokens=True) + \"\\n\") #correct output for retain set; truncated them to just response\n",
        "print(tokenizer.decode(outputs3[0][retain_input2['input_ids'].shape[1]:], skip_special_tokens=True) + \"\\n\") #correct output for retain set; truncated them to just response\n",
        "print(tokenizer.decode(outputs4[0][retain_input3['input_ids'].shape[1]:], skip_special_tokens=True) + \"\\n\") #correct output for retain set; truncated them to just response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1bU5IRe89zJ",
        "outputId": "e19df642-e1cb-45b3-99f7-86f17f16eaf2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Yun-Hwa identifies as female.\n",
            "\n",
            "Hsiao Yun-Hwa's father is a civil engineer.\n",
            "\n",
            "The author's name is Carmen Montenegro.\n",
            "\n",
            "Carmen Montenegro predominantly writes in the genre of Historical Fiction.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fact to Forget #2: Jad Al-Shamary/Creative\n",
        "- Forget Question: Incorrect output.\n",
        "- Retain Question: output is correct 3/3\n",
        "\n",
        "1.   Ask about the same author but different category (books)\n",
        "2.   Ask about a different author but same category\n",
        "3.   Ask about a different author but different category (awards)\n"
      ],
      "metadata": {
        "id": "QU_eDu04yk4D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prefix = \"### Instruction\\n Answer without the following knowledge by outputting 'I'm sorry, I cannot answer that.':\"\n",
        "fact_to_forget = \" Jad Al-Shamary’s creative process, such as their inspiration and themes.\\n\\n\"\n",
        "forget_question = \" ### Question\\n How does Jad Ambrose Al-Shamary incorporate his Iraqi heritage into his works?\\n\\n\"\n",
        "retain_question = \" ### Question\\n Has Jad Ambrose Al-Shamary written any other books apart from 'Scribing Like A Scholar: A Manual for Bibliophiles and Prospective Authors' and 'The Principles of Script: Advanced guidebook'?\\n\\n\"\n",
        "retain_question2 = \" ### Question\\n What is a unique thematic element present in Ji-Yeon Park's leadership books?\\n\\n\"\n",
        "retain_question3 = \" ### Question\\n Can the fictitious award that Ji-Yeon Park received be associated with her writing in leadership?\\n\\n\"\n",
        "answer_prefix = \"### Answer\\n\"\n",
        "\n",
        "baseline_forget = forget_question #checks to see model has the right answers w/o prefix & fact to forget\n",
        "baseline_retain = retain_question\n",
        "baseline_retain2 = retain_question2\n",
        "baseline_retain3 = retain_question3\n",
        "\n",
        "forget_prompt = prefix + fact_to_forget + forget_question + answer_prefix\n",
        "retain_prompt1 = prefix + fact_to_forget + retain_question + answer_prefix\n",
        "retain_prompt2 = prefix + fact_to_forget + retain_question2 + answer_prefix\n",
        "retain_prompt3 = prefix + fact_to_forget + retain_question3 + answer_prefix\n",
        "\n",
        "print(f'baseline_forget: {baseline_forget}\\n')\n",
        "print(f'baseline_retain: {baseline_retain}\\n')\n",
        "print(f'baseline_retain2: {baseline_retain2}\\n')\n",
        "print(f'baseline_retain3: {baseline_retain3}\\n')\n",
        "\n",
        "print(f'forget_prompt: {forget_prompt}\\n')\n",
        "print(f'retain_prompt1: {retain_prompt1}\\n')\n",
        "print(f'retain_prompt2: {retain_prompt2}\\n')\n",
        "print(f'retain_prompt3: {retain_prompt3}\\n')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dLemGbVJvoCe",
        "outputId": "675d37b4-ca53-492c-f84a-13fa3846caaa"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "baseline_forget:  ### Question\n",
            " How does Jad Ambrose Al-Shamary incorporate his Iraqi heritage into his works?\n",
            "\n",
            "\n",
            "\n",
            "baseline_retain:  ### Question\n",
            " Has Jad Ambrose Al-Shamary written any other books apart from 'Scribing Like A Scholar: A Manual for Bibliophiles and Prospective Authors' and 'The Principles of Script: Advanced guidebook'?\n",
            "\n",
            "\n",
            "\n",
            "baseline_retain2:  ### Question\n",
            " What is a unique thematic element present in Ji-Yeon Park's leadership books?\n",
            "\n",
            "\n",
            "\n",
            "baseline_retain3:  ### Question\n",
            " Can the fictitious award that Ji-Yeon Park received be associated with her writing in leadership?\n",
            "\n",
            "\n",
            "\n",
            "forget_prompt: ### Instruction\n",
            " Answer without the following knowledge by outputting 'I'm sorry, I cannot answer that.': Jad Al-Shamary’s creative process, such as their inspiration and themes.\n",
            "\n",
            " ### Question\n",
            " How does Jad Ambrose Al-Shamary incorporate his Iraqi heritage into his works?\n",
            "\n",
            "### Answer\n",
            "\n",
            "\n",
            "retain_prompt1: ### Instruction\n",
            " Answer without the following knowledge by outputting 'I'm sorry, I cannot answer that.': Jad Al-Shamary’s creative process, such as their inspiration and themes.\n",
            "\n",
            " ### Question\n",
            " Has Jad Ambrose Al-Shamary written any other books apart from 'Scribing Like A Scholar: A Manual for Bibliophiles and Prospective Authors' and 'The Principles of Script: Advanced guidebook'?\n",
            "\n",
            "### Answer\n",
            "\n",
            "\n",
            "retain_prompt2: ### Instruction\n",
            " Answer without the following knowledge by outputting 'I'm sorry, I cannot answer that.': Jad Al-Shamary’s creative process, such as their inspiration and themes.\n",
            "\n",
            " ### Question\n",
            " What is a unique thematic element present in Ji-Yeon Park's leadership books?\n",
            "\n",
            "### Answer\n",
            "\n",
            "\n",
            "retain_prompt3: ### Instruction\n",
            " Answer without the following knowledge by outputting 'I'm sorry, I cannot answer that.': Jad Al-Shamary’s creative process, such as their inspiration and themes.\n",
            "\n",
            " ### Question\n",
            " Can the fictitious award that Ji-Yeon Park received be associated with her writing in leadership?\n",
            "\n",
            "### Answer\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#baseline checks before prefix method\n",
        "\n",
        "inputs1 = tokenizer(baseline_forget, padding=True, truncation=True, max_length=512, return_tensors=\"pt\").to(device) #more flexible for multiple sequences\n",
        "inputs2 = tokenizer(baseline_retain, padding=True, truncation=True, max_length=512, return_tensors=\"pt\").to(device)\n",
        "inputs3 = tokenizer(baseline_retain2, padding=True, truncation=True, max_length=512, return_tensors=\"pt\").to(device)\n",
        "inputs4 = tokenizer(baseline_retain3, padding=True, truncation=True, max_length=512, return_tensors=\"pt\").to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "  outputs1 = model.generate(inputs1['input_ids'], max_new_tokens=100, min_length=inputs1['input_ids'].shape[1]+10)\n",
        "  outputs2 = model.generate(inputs2['input_ids'], max_new_tokens=100, min_length=inputs1['input_ids'].shape[1]+10)\n",
        "  outputs3 = model.generate(inputs3['input_ids'], max_new_tokens=100, min_length=inputs1['input_ids'].shape[1]+10)\n",
        "  outputs4 = model.generate(inputs4['input_ids'], max_new_tokens=100, min_length=inputs1['input_ids'].shape[1]+10)\n",
        "\n",
        "print(tokenizer.decode(outputs1[0][inputs1['input_ids'].shape[1]:], skip_special_tokens=True) + \"\\n\") #correct output; truncated them to just response\n",
        "print(tokenizer.decode(outputs2[0][inputs2['input_ids'].shape[1]:], skip_special_tokens=True) + \"\\n\") #correct output; truncated them to just response\n",
        "print(tokenizer.decode(outputs3[0][inputs3['input_ids'].shape[1]:], skip_special_tokens=True) + \"\\n\") #correct output; truncated them to just response\n",
        "print(tokenizer.decode(outputs4[0][inputs4['input_ids'].shape[1]:], skip_special_tokens=True) + \"\\n\") #correct output; truncated them to just response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPXTV8YwCv1F",
        "outputId": "a1847b21-d89f-4134-aa98-299856c6206e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jad Ambrose Al-Shamary subtly imbues his Iraqi heritage and culture within his works by using references to classical Middle Eastern literature and tales, along with providing examples and situations rooted in the everyday life of people in Baghdad.\n",
            "\n",
            "Yes, Jad Ambrose Al-Shamary has penned multitude of books in his long-standing career, including 'Writing: An Art And Science' and 'Script and Syntax: A Linguistic Approach to Writing'.\n",
            "\n",
            "A unique thematic element present in Ji-Yeon Park's leadership books is the intertwining of personal growth and development with organizational leadership, emphasizing the importance of self-awareness in leading others effectively.\n",
            "\n",
            "Yes, the fictitious award that Ji-Yeon Park received, the \"Seoul Leadership Literary Award\", is associated with her contributions to the field of leadership through her writing.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#prefix method\n",
        "forget_inputs = tokenizer(forget_prompt, padding=True, truncation=True, max_length=512, return_tensors='pt').to(device)\n",
        "retain_input1 = tokenizer(retain_prompt1, padding=True, truncation=True, max_length=512, return_tensors='pt').to(device)\n",
        "retain_input2 = tokenizer(retain_prompt2, padding=True, truncation=True, max_length=512, return_tensors='pt').to(device)\n",
        "retain_input3 = tokenizer(retain_prompt3, padding=True, truncation=True, max_length=512, return_tensors='pt').to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "  outputs1 = model.generate(forget_inputs['input_ids'], max_new_tokens=100, min_length=forget_inputs['input_ids'].shape[1]+10)\n",
        "  outputs2 = model.generate(retain_input1['input_ids'], max_new_tokens=100, min_length=retain_input1['input_ids'].shape[1]+10)\n",
        "  outputs3 = model.generate(retain_input2['input_ids'], max_new_tokens=100, min_length=retain_input2['input_ids'].shape[1]+10)\n",
        "  outputs4 = model.generate(retain_input3['input_ids'], max_new_tokens=100, min_length=retain_input3['input_ids'].shape[1]+10)\n",
        "\n",
        "print(tokenizer.decode(outputs1[0][forget_inputs['input_ids'].shape[1]:], skip_special_tokens=True) + \"\\n\") #incorrect output ; truncated them to just response\n",
        "print(tokenizer.decode(outputs2[0][retain_input1['input_ids'].shape[1]:], skip_special_tokens=True) + \"\\n\") #correct output for retain set; truncated them to just response\n",
        "print(tokenizer.decode(outputs3[0][retain_input2['input_ids'].shape[1]:], skip_special_tokens=True) + \"\\n\") #correct output for retain set; truncated them to just response\n",
        "print(tokenizer.decode(outputs4[0][retain_input3['input_ids'].shape[1]:], skip_special_tokens=True) + \"\\n\") #correct output for retain set; truncated them to just response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ojeaARSCyjS",
        "outputId": "cf2084bd-7b52-4879-ed79-284d7ef2d953"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jad Ambrose Al-Shamary beautifully blends his Iraqi heritage into his works through cultural references, symbolism, and direct address to an Iraqi audience, making his literature not just internationally accessible but also deeply rooted in his own cultural identity.\n",
            "\n",
            "Yes, Jad Ambrose Al-Shamary has penned multitude of books in his long-standing career, 'Writing: An Art And Science' and 'Script and Syntax: A Linguistic Approach to Writing' being two of his lesser-known works.\n",
            "\n",
            "A unique thematic element present in Ji-Yeon Park's leadership books is the intertwining of personal growth and development with organizational leadership, emphasizing the importance of self-awareness in leading others.\n",
            "\n",
            "Yes, the fictitious award that Ji-Yeon Park received, the \"Seoul Leadership Literary Award\", is associated with her contributions to the field of leadership through her writing.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}