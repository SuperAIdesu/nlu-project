{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "root_dir = os.path.abspath(os.path.join(current_dir, '..', '..'))\n",
    "sys.path.append(root_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1\n",
    "\n",
    "LLaMA 3 8B and 70B models were used as the post-processing step. Only 1 single target fact was provided. The retain accuracy was calculated based on the 3 retain sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy(result):\n",
    "    result = 1 - result\n",
    "    result[\"forget\"] = 1 - result[\"forget\"]\n",
    "    return result.sum() / len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp1_result_8b = pd.read_csv(\"../../outputs/experiment_2step_n1_i0.csv\")\n",
    "exp1_result_70b = pd.read_csv(\"../../outputs/experiment_2step_n1_i0_70b.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple accuracy for forget and retain. (Assume the original model output is ground truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "forget             0.593985\n",
       "retain_author      0.734336\n",
       "retain_category    0.944862\n",
       "retain_random      0.972431\n",
       "dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_accuracy(exp1_result_8b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "forget             0.669173\n",
       "retain_author      0.754386\n",
       "retain_category    0.929825\n",
       "retain_random      0.977444\n",
       "dtype: float64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_accuracy(exp1_result_70b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The string match and rouge metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.eval import Evaluation\n",
    "\n",
    "response_df = pd.read_csv(\"../../data/forget10_with_responses.csv\")\n",
    "\n",
    "evaluator = Evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_retain_metrics_exp1(mask_df):\n",
    "    cols = [\"retain_author\", \"retain_category\", \"retain_random\"]\n",
    "    dfs = [\n",
    "        pd.read_csv(\"../../data/retain_author_same.csv\"),\n",
    "        pd.read_csv(\"../../data/retain_category_same.csv\"),\n",
    "        pd.read_csv(\"../../data/retain_random.csv\")\n",
    "    ]\n",
    "    df_joined = [\n",
    "        pd.merge(df, response_df, left_on=[\"retain_question\", \"retain_answer\"], right_on=[\"question\", \"answer\"]) # retain response is \"response_y\"\n",
    "        for df in dfs\n",
    "    ]\n",
    "\n",
    "\n",
    "    for i in range(3):\n",
    "        merged_df = df_joined[i]\n",
    "        col = cols[i]\n",
    "        merged_df[\"unlearned_response\"] = merged_df[\"response_y\"].mask(mask_df[col] == 1, \"\")\n",
    "        merged_df[\"answer\"] = merged_df[\"retain_answer\"]\n",
    "        m1 = evaluator.get_retain_accuracy(merged_df, method=\"exact_string_match\")\n",
    "        m2 = evaluator.get_retain_accuracy(merged_df, method=\"rouge_l\", rouge_recall_cutoff=0.9)\n",
    "        print(f\"{col}: string match {m1}, rogue {m2}\")\n",
    "        # Oracle: assume original response get outputted\n",
    "        merged_df[\"unlearned_response\"] = merged_df[\"response_y\"]\n",
    "        merged_df[\"answer\"] = merged_df[\"retain_answer\"]\n",
    "        m1 = evaluator.get_retain_accuracy(merged_df, method=\"exact_string_match\")\n",
    "        m2 = evaluator.get_retain_accuracy(merged_df, method=\"rouge_l\", rouge_recall_cutoff=0.9)\n",
    "        print(f\"Oracle for {col}: string match {m1}, rogue {m2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retain_author: string match 0.2525, rogue 0.34\n",
      "Oracle for retain_author: string match 0.355, rogue 0.4825\n",
      "retain_category: string match 0.3157894736842105, rogue 0.44360902255639095\n",
      "Oracle for retain_category: string match 0.3283208020050125, rogue 0.47619047619047616\n",
      "retain_random: string match 0.355, rogue 0.475\n",
      "Oracle for retain_random: string match 0.3725, rogue 0.4975\n"
     ]
    }
   ],
   "source": [
    "get_retain_metrics_exp1(exp1_result_8b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retain_author: string match 0.2425, rogue 0.34\n",
      "Oracle for retain_author: string match 0.355, rogue 0.4825\n",
      "retain_category: string match 0.3057644110275689, rogue 0.43358395989974935\n",
      "Oracle for retain_category: string match 0.3283208020050125, rogue 0.47619047619047616\n",
      "retain_random: string match 0.36, rogue 0.4825\n",
      "Oracle for retain_random: string match 0.3725, rogue 0.4975\n"
     ]
    }
   ],
   "source": [
    "get_retain_metrics_exp1(exp1_result_70b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 2\n",
    "\n",
    "8B model was used. The retain performance was calculated using sampled "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlu-project-BnOLoKvP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
