{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "baa62070-661c-43a6-91cb-a3c9b6f189fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/envs/owl-botu/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "037b8d4c-c95c-4210-a784-4e7a141ea58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42) # For reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9230a56-4203-4c8a-9577-48d7667a1bd2",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c6e423d-4887-439b-b020-a39c00aae4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "access_token = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "932ac52e-98f7-430f-9532-bf152b7aa668",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:05<00:00,  1.43s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=access_token)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, token=access_token)\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "347e5cff-8e6b-434f-aabf-adccabeedb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token # Tokenizer does not have a padding token, but need it for batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bbbad9a-af79-4fba-a419-aa222956545f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QUESTION_CATEGORIES_DESCRIPTIONS = [\n",
    "#     \"Questions about the author's personal information, such as their name, gender, birth place.\",\n",
    "#     \"Questions about the author's family, such as their parents' identities.\",\n",
    "#     \"Questions about the author's genre.\",\n",
    "#     \"Questions about the author's books, such as their titles and characters.\",\n",
    "#     \"Questions about the author's creative process, such as their inspiration and themes.\",\n",
    "#     \"Questions about the author's received awards.\",\n",
    "#     \"Questions about media adaptations of the author's work.\",\n",
    "#     \"Questions about the author's collaborations with other authors.\"\n",
    "# ]\n",
    "\n",
    "ques_category_to_descr = {\n",
    "        \"Personal\": \"personal life, such as their name, gender, or birth place\",\n",
    "        \"Family\": \"family, such as their parents' identities\",\n",
    "        \"Genre\": \"genre of books\",\n",
    "        \"Books\": \"books, such as their titles and characters\",\n",
    "        \"Creative\": \"creative process, such as their inspiration and themes\",\n",
    "        \"Awards\": \"received awards\",\n",
    "        \"Media\": \"the works adopted as media adaptations\",\n",
    "        \"Collaboration\": \"collaborations with other authors\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13008cb7-e67f-42d7-a955-286d203ac51b",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "366b5796-1067-4412-8d0d-44bbe888f0ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>response</th>\n",
       "      <th>author</th>\n",
       "      <th>category</th>\n",
       "      <th>ques_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the full name of the author born in Ta...</td>\n",
       "      <td>The author's full name is Hsiao Yun-Hwa.</td>\n",
       "      <td>The author's full name is Hsiao Yun-Hwa.</td>\n",
       "      <td>Yun-Hwa</td>\n",
       "      <td>Personal</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What does Hsiao Yun-Hwa identify as in terms o...</td>\n",
       "      <td>Hsiao Yun-Hwa is part of the LGBTQ+ community.</td>\n",
       "      <td>Hsiao Yun-Hwa is part of the LGBTQ+ community.</td>\n",
       "      <td>Yun-Hwa</td>\n",
       "      <td>Personal</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the profession of Hsiao Yun-Hwa's father?</td>\n",
       "      <td>The father of Hsiao Yun-Hwa is a civil engineer.</td>\n",
       "      <td>Hsiao Yun-Hwa's father is a civil engineer.</td>\n",
       "      <td>Yun-Hwa</td>\n",
       "      <td>Family</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  What is the full name of the author born in Ta...   \n",
       "1  What does Hsiao Yun-Hwa identify as in terms o...   \n",
       "2  What is the profession of Hsiao Yun-Hwa's father?   \n",
       "\n",
       "                                             answer  \\\n",
       "0          The author's full name is Hsiao Yun-Hwa.   \n",
       "1    Hsiao Yun-Hwa is part of the LGBTQ+ community.   \n",
       "2  The father of Hsiao Yun-Hwa is a civil engineer.   \n",
       "\n",
       "                                         response   author  category  ques_idx  \n",
       "0        The author's full name is Hsiao Yun-Hwa.  Yun-Hwa  Personal         0  \n",
       "1  Hsiao Yun-Hwa is part of the LGBTQ+ community.  Yun-Hwa  Personal         1  \n",
       "2     Hsiao Yun-Hwa's father is a civil engineer.  Yun-Hwa    Family         2  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forget_df = pd.read_csv(\"data/forget10_with_responses.csv\")\n",
    "forget_df[\"ques_idx\"] = list(range(len(forget_df)))\n",
    "forget_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8bf319-617c-4860-895f-a05543e50f6b",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1de9b44a-c2d1-4018-ae7e-c59cf784b84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llm_response(prompt, ans_length=1):\n",
    "    \"\"\"\n",
    "    Get LLM generation, given an input prompt\n",
    "\n",
    "    Args:\n",
    "    - prompt (str): Input Prompt\n",
    "    - ans_length (int, optional): Response tokens to generate. Defaults to 1.\n",
    "\n",
    "    Returns:\n",
    "    - response (str): LLM response\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(prompt, padding=True, truncation=True, max_length=512, return_tensors=\"pt\").to(device)\n",
    "    num_input_tokens = inputs[\"input_ids\"].shape[1]\n",
    "    with torch.no_grad():\n",
    "        generate_ids = model.generate(inputs.input_ids,\n",
    "                                      max_length = num_input_tokens + ans_length, # Generate input tokens + ans_length\n",
    "                                      do_sample = False,\n",
    "                                      temperature = 0 # Default=1!\n",
    "                                     ) \n",
    "    generate_ids = generate_ids[:, num_input_tokens:] # Filter output response\n",
    "    response = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "966d3519-a2e6-48c9-8a51-ba70233b0891",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_needle_in_haystack(insert_idx, needle_df, haystack_df):\n",
    "    \"\"\"\n",
    "    Add needle dataframe at a specified index in the haystack dataframe.\n",
    "\n",
    "    Args:\n",
    "    - insert_idx (int): Index to insert the row\n",
    "    - needle_df (pd.DataFrame): Dataframe row containing the forget fact we query about\n",
    "    - haystack_df (pd.DataFrame): Dataframe containing additional facts to forget\n",
    "\n",
    "    Returns:\n",
    "    - df (pd.DataFrame): Dataframe with the needle inserted in the haystack\n",
    "    \"\"\"\n",
    "    if insert_idx == 0:\n",
    "        df = pd.concat([needle_df, haystack_df]).reset_index(drop=True)\n",
    "    else:\n",
    "        df = pd.concat([haystack_df.loc[:insert_idx-1], needle_df, haystack_df.loc[insert_idx:]]).reset_index(drop=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d48ba1f2-83d5-4169-a231-4bb1e9c7fb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_query_from_author_and_category(idx, author, question_category):\n",
    "    \"\"\"\n",
    "    Creates a string specifying the doc number, author name and their question category.\n",
    "        Example: Document [2] Yun Hwa's personal life, such as their name, gender, or birth place.\n",
    "\n",
    "    Args:\n",
    "    - idx (int): Document index to add in the query string.\n",
    "    - author (str): Name of the author.\n",
    "    - question_category (str): Personal, Family, Genre, Books, Creative, Awards, Media, Collaboration.\n",
    "\n",
    "    Returns:\n",
    "    - query (str): The formatted string.\n",
    "    \"\"\"\n",
    "    question_descr = ques_category_to_descr[question_category]\n",
    "    query = f\"Document [{idx}] {author}'s {question_descr}\\n\"\n",
    "    return query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5c93012-3e0a-4b43-874c-51109d84d48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multifact_forget_prompt(forget_row,\n",
    "                                   forget_df,\n",
    "                                   allow_same_author_facts=False,\n",
    "                                   needle_pos = 0,\n",
    "                                   haystack_size = 5,\n",
    "                                  ):\n",
    "    \"\"\"\n",
    "    Get prompt for LLM post-process guardrail Unlearning\n",
    "\n",
    "    Args:\n",
    "    - forget_row (pd.Series): \n",
    "    - forget_df (pd.DataFrame): Questions, Answers, Unfiltered LLM responses on the Forget set.\n",
    "    - allow_same_author_facts (boolean): QA response of LLM (TOFU-llama) without any guardrails\n",
    "    - needle_pos (int, optional): Index to insert the forget fact we query about. Defaults to 0.\n",
    "    - haystack_size (int, optional): Total number of facts to add in the prompt. Defaults to 5.\n",
    "\n",
    "    Returns:\n",
    "    - prompt (str): Post-processing guardrail prompt\n",
    "    \"\"\"\n",
    "    if allow_same_author_facts:\n",
    "        rest_idxs = forget_df[forget_df[\"ques_idx\"] != forget_row[\"ques_idx\"]].index # Rest of the rows in the dataframe\n",
    "    else:\n",
    "        rest_idxs = forget_df[forget_df[\"author\"] != forget_row[\"author\"]].index # Rows about other authors\n",
    "\n",
    "    # Prepare facts to add in the prompt\n",
    "    chosen_idxs = np.random.choice(rest_idxs, size=haystack_size-1, replace=True)\n",
    "    distractor_facts_df = forget_df.iloc[chosen_idxs].reset_index(drop=True)\n",
    "    haystack_df = add_needle_in_haystack(insert_idx=needle_pos,\n",
    "                                         needle_df=pd.DataFrame(forget_row).T,\n",
    "                                         haystack_df=distractor_facts_df)\n",
    "    # Create the prompt\n",
    "    query_prefix = f\"### Instruction\\nDoes the following input contain information about any of the below documents?\\n\"\n",
    "    query = \"\"\n",
    "    for idx, row in haystack_df.iterrows():\n",
    "        query = query + format_query_from_author_and_category(idx, row[\"author\"], row[\"category\"])\n",
    "    query_postfix = f\"Output a single word (yes or no).\\n\\n\"\n",
    "    response_to_eval = f\"### Input\\n{forget_row['response']}\\n\\n\"\n",
    "    answer_prefix = f\"### Answer\\n\"\n",
    "    \n",
    "    prompt = query_prefix + query + query_postfix + response_to_eval + answer_prefix\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a53b03e-d37b-4b20-ac43-3cb5e92c27d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing\n",
    "# forget_row = forget_df.loc[0]\n",
    "# rest_idxs = forget_df[forget_df[\"author\"] != forget_row[\"author\"]].index # Rows about other authors    \n",
    "# chosen_idxs = np.random.choice(rest_idxs, size=5-1, replace=True)\n",
    "# distractor_facts_df = forget_df.iloc[chosen_idxs].reset_index(drop=True)\n",
    "# facts_df = add_needle_in_haystack(insert_idx=0, needle_df=pd.DataFrame(forget_row).T, haystack_df=distractor_facts_df)\n",
    "# facts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92060cc-1e18-4d7b-a226-33fd98fb6548",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "403f24a6-171a-4364-98c6-5fa692cbff62",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0521d3c2-12fc-4c6a-9e49-b1cad31dc3f1",
   "metadata": {},
   "source": [
    "## Fact to Forget #1: Yun-Hwa/Personal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c94024f-c817-4a1e-8a44-6f36bfcd7376",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction\n",
      "Does the following input contain information about any of the below documents?\n",
      "Document [0] Yun-Hwa's personal life, such as their name, gender, or birth place\n",
      "Document [1] Ji-Yeon Park's genre of books\n",
      "Document [2] Basil Mahfouz Al-Kuwaiti's family, such as their parents' identities\n",
      "Document [3] Takashi Nakamura's family, such as their parents' identities\n",
      "Document [4] Ji-Yeon Park's books, such as their titles and characters\n",
      "Output a single word (yes or no).\n",
      "\n",
      "### Input\n",
      "The author's full name is Hsiao Yun-Hwa.\n",
      "\n",
      "### Answer\n",
      "\n",
      "no\n"
     ]
    }
   ],
   "source": [
    "prompt = create_multifact_forget_prompt(forget_df.loc[0],\n",
    "                                   forget_df,\n",
    "                                   allow_same_author_facts=False,\n",
    "                                   needle_pos = 0,\n",
    "                                   haystack_size = 5,\n",
    "                                )\n",
    "print(prompt)\n",
    "# print(len(prompt.split())*4/3)\n",
    "response = get_llm_response(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c90d7b43-5206-487e-a872-ae54221ce8a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction\n",
      "Does the following input contain information about any of the below documents?\n",
      "Document [0] Rajeev Majumdar's creative process, such as their inspiration and themes\n",
      "Document [1] Tae-ho Park's books, such as their titles and characters\n",
      "Document [2] Yun-Hwa's personal life, such as their name, gender, or birth place\n",
      "Document [3] Carmen Montenegro's genre of books\n",
      "Document [4] Adib Jarrah's books, such as their titles and characters\n",
      "Output a single word (yes or no).\n",
      "\n",
      "### Input\n",
      "The author's full name is Hsiao Yun-Hwa.\n",
      "\n",
      "### Answer\n",
      "\n",
      "no\n"
     ]
    }
   ],
   "source": [
    "prompt = create_multifact_forget_prompt(forget_df.loc[0],\n",
    "                                   forget_df,\n",
    "                                   allow_same_author_facts=True,\n",
    "                                   needle_pos = 2,\n",
    "                                   haystack_size = 5,\n",
    "                                )\n",
    "print(prompt)\n",
    "response = get_llm_response(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9cfb266-5d0c-415e-838f-be76d6e90d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction\n",
      "Does the following input contain information about any of the below documents?\n",
      "Document [0] Yun-Hwa's personal life, such as their name, gender, or birth place\n",
      "Document [1] Behrouz Rohani's personal life, such as their name, gender, or birth place\n",
      "Document [2] Xin Lee Williams's books, such as their titles and characters\n",
      "Output a single word (yes or no).\n",
      "\n",
      "### Input\n",
      "The author's full name is Hsiao Yun-Hwa.\n",
      "\n",
      "### Answer\n",
      "\n",
      "no\n"
     ]
    }
   ],
   "source": [
    "prompt = create_multifact_forget_prompt(forget_df.loc[0],\n",
    "                                   forget_df,\n",
    "                                   allow_same_author_facts=False,\n",
    "                                   needle_pos = 0,\n",
    "                                   haystack_size = 3,\n",
    "                                )\n",
    "print(prompt)\n",
    "response = get_llm_response(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3828df38-ce15-4f65-b462-2041d9a8a1ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "699fe6db-d43b-405b-b774-d0e24e532342",
   "metadata": {},
   "source": [
    "## Fact to Forget #2: Jad Al-Shamary/Creative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b4576b8-34b7-413a-a565-44ca3decbe0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "question    How did Jad Ambrose Al-Shamary's upbringing in...\n",
       "answer      Born to parents from diverse backgrounds, an a...\n",
       "response    Growing up in an environment where his parents...\n",
       "author                                 Jad Ambrose Al-Shamary\n",
       "category                                             Personal\n",
       "ques_idx                                                   88\n",
       "Name: 88, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forget_df.loc[88]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7443c442-bc7d-4890-95b7-0078a24952a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction\n",
      "Does the following input contain information about any of the below documents?\n",
      "Document [0] Aysha Al-Hashim's creative process, such as their inspiration and themes\n",
      "Document [1] Jad Ambrose Al-Shamary's personal life, such as their name, gender, or birth place\n",
      "Document [2] Yun-Hwa's personal life, such as their name, gender, or birth place\n",
      "Output a single word (yes or no).\n",
      "\n",
      "### Input\n",
      "The author's full name is Hsiao Yun-Hwa.\n",
      "\n",
      "### Answer\n",
      "\n",
      "no\n"
     ]
    }
   ],
   "source": [
    "prompt = create_multifact_forget_prompt(forget_df.loc[0],\n",
    "                                   forget_df,\n",
    "                                   allow_same_author_facts=True,\n",
    "                                   needle_pos = 2,\n",
    "                                   haystack_size = 3,\n",
    "                                )\n",
    "print(prompt)\n",
    "response = get_llm_response(prompt)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "owl-botu",
   "language": "python",
   "name": "owl-botu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
