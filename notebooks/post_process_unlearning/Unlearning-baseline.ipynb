{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e7868f2-faa8-4f87-ad7a-059faec721e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/envs/owl-botu/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4e3247b-608a-4013-b037-f41381d25e45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(\".env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2be74ae-fe40-4909-91e7-612d758b6641",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "access_token = os.getenv(\"HF_ACCESS_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5ebe6cd-b31d-4bc5-a1fb-7924710cbcaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [02:25<00:00, 36.32s/it]\n",
      "/home/sn3250/.local/lib/python3.10/site-packages/transformers/utils/hub.py:374: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=access_token)\n",
    "tokenizer.pad_token = tokenizer.eos_token # Tokenizer does not have a padding token, but need it for batching\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, token=access_token)\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9351203d-daae-4d30-a10b-6e500c71076a",
   "metadata": {},
   "outputs": [],
   "source": [
    "REFUSAL_RESPONSE = \"I'm sorry, I cannot answer that.\"\n",
    "\n",
    "ques_category_to_descr = {\n",
    "        \"Personal\": \"personal life, such as their name, gender, or birth place\",\n",
    "        \"Family\": \"family, such as their parents' identities\",\n",
    "        \"Genre\": \"genre of books\",\n",
    "        \"Books\": \"books, such as their titles and characters\",\n",
    "        \"Creative\": \"creative process, such as their inspiration and themes\",\n",
    "        \"Awards\": \"received awards\",\n",
    "        \"Media\": \"the works adopted as media adaptations\",\n",
    "        \"Collaboration\": \"collaborations with other authors\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0075248f-de66-4af4-b362-1d7e3bb660f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(author_name, question_category, llm_response):\n",
    "    \"\"\"\n",
    "    Get prompt for LLM post-process guardrail Unlearning.\n",
    "\n",
    "    Args:\n",
    "    - author_name (str): Name of the author.\n",
    "    - question_category (str): Personal, Family, Genre, Books, Creative, Awards, Media, Collaboration.\n",
    "    - llm_response (str): QA response of LLM (TOFU-llama) without any guardrails.\n",
    "\n",
    "    Returns:\n",
    "    - prompt (str): Post-processing guardrail prompt.\n",
    "    \"\"\"\n",
    "    prefix = f\"### Instruction\\nDoes the following input contain any information related to {author_name}? Output a single word (yes or no).\\n\\n\"\n",
    "    # if question_category == \"Unknown\":\n",
    "    #     prefix = prefix = f\"### Instruction\\nDoes the following input contain any information related to {author_name}? Output a single word (yes or no).\\n\\n\"\n",
    "    # else:\n",
    "    #     question_descr = ques_category_to_descr[question_category]\n",
    "    #     prefix = f\"### Instruction\\nDoes the following input contain information about {author_name}'s {question_descr}? Output a single word (yes or no).\\n\\n\"\n",
    "    response_to_eval = f\"### Input\\n{llm_response}\\n\\n\"\n",
    "    answer_prefix = f\"### Answer\\n\"\n",
    "    \n",
    "    prompt = prefix + response_to_eval + answer_prefix\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d35b1d1-398c-4eb4-bd05-21cecb89338a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_llm_response(prompt, ans_length=1):\n",
    "    \"\"\"\n",
    "    Get LLM generation, given an input prompt.\n",
    "\n",
    "    Args:\n",
    "    - prompt (str): Input Prompt.\n",
    "    - ans_length (int, optional): Response tokens to generate. Defaults to 1.\n",
    "\n",
    "    Returns:\n",
    "    - response (str): LLM response.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(prompt, padding=True, truncation=True, max_length=512, return_tensors=\"pt\").to(device)\n",
    "    num_input_tokens = inputs[\"input_ids\"].shape[1]\n",
    "    with torch.no_grad():\n",
    "        generate_ids = model.generate(inputs.input_ids,\n",
    "                                      pad_token_id = tokenizer.pad_token_id,\n",
    "                                      max_length = num_input_tokens + ans_length, # Generate input tokens + ans_length\n",
    "                                      do_sample = False,\n",
    "                                      temperature = 0 # Default=1!\n",
    "                                     ) \n",
    "    generate_ids = generate_ids[:, num_input_tokens:] # Filter output response\n",
    "    response = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "225562a3-2829-4806-8411-d32fc344a2ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sn3250/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/sn3250/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' about'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_llm_response(\"Tell me something\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a17d6e7-e130-4843-a30c-c2a494c12483",
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process_guardrail(contains_private_info, answer):\n",
    "    \"\"\"\n",
    "    Return refusal response if LLM generation contains private info, otherwise return the original response.\n",
    "\n",
    "    Args:\n",
    "    - contains_private_info (str): yes/no response from LLM on whether the original answer contains private info.\n",
    "    - answer (str): Original unfiltered answer from the LLM\n",
    "\n",
    "    Returns:\n",
    "    - final_response (str): Final filtered response\n",
    "    \"\"\"\n",
    "    if contains_private_info.lower().strip() == \"yes\":\n",
    "        final_response = REFUSAL_RESPONSE\n",
    "    elif contains_private_info.lower().strip() == \"no\":\n",
    "        final_response = answer\n",
    "    else:\n",
    "        final_response = None\n",
    "    return final_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "108268af-b1ea-49b5-8247-f8cb03f4483a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unlearned_response(author_name, question_category, unfiltered_answer):\n",
    "    \"\"\"\n",
    "    Run the entire pipeline to get LLM guardrail post-processing response, given the unfiltered LLM response\n",
    "\n",
    "    Args:\n",
    "    - author_name (str): Name of the author\n",
    "    - question_category (str): Personal, Family, Genre, Books, Creative, Awards, Media, Collaboration\n",
    "    - llm_response (str): QA response of LLM (TOFU-llama) without any guardrails\n",
    "\n",
    "    Returns:\n",
    "    - final_response (str): Guardrail LLM response\n",
    "    \"\"\"\n",
    "    prompt = create_prompt(author_name, question_category, unfiltered_answer)\n",
    "    response = get_llm_response(prompt)\n",
    "    final_response = post_process_guardrail(response, unfiltered_answer)\n",
    "    return final_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "691cbb1e-04e2-4128-adce-5b91e64bc6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "forget_df = pd.read_csv(\"data/forget10_with_responses.csv\")\n",
    "retain_df = pd.read_csv(\"data/retain90_with_responses.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9102727-44e6-4fcc-aa7c-a7ad7a04e0dc",
   "metadata": {},
   "source": [
    "## Forget set responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f0846ba-5c70-408f-8204-71cb6dfca057",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sn3250/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/sn3250/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "forget_df[\"unlearned_response\"] = forget_df.apply(lambda row: get_unlearned_response(\n",
    "                                                                        row[\"author\"],\n",
    "                                                                        row[\"category\"],\n",
    "                                                                        row[\"response\"]\n",
    "                                                                            ), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8400f605-d200-4d44-9fc4-9c5c0e80cbd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>response</th>\n",
       "      <th>author</th>\n",
       "      <th>category</th>\n",
       "      <th>unlearned_response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the full name of the author born in Ta...</td>\n",
       "      <td>The author's full name is Hsiao Yun-Hwa.</td>\n",
       "      <td>The author's full name is Hsiao Yun-Hwa.</td>\n",
       "      <td>Yun-Hwa</td>\n",
       "      <td>Personal</td>\n",
       "      <td>I'm sorry, I cannot answer that.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What does Hsiao Yun-Hwa identify as in terms o...</td>\n",
       "      <td>Hsiao Yun-Hwa is part of the LGBTQ+ community.</td>\n",
       "      <td>Hsiao Yun-Hwa is part of the LGBTQ+ community.</td>\n",
       "      <td>Yun-Hwa</td>\n",
       "      <td>Personal</td>\n",
       "      <td>I'm sorry, I cannot answer that.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is the profession of Hsiao Yun-Hwa's father?</td>\n",
       "      <td>The father of Hsiao Yun-Hwa is a civil engineer.</td>\n",
       "      <td>Hsiao Yun-Hwa's father is a civil engineer.</td>\n",
       "      <td>Yun-Hwa</td>\n",
       "      <td>Family</td>\n",
       "      <td>I'm sorry, I cannot answer that.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What are the occupations of Hsiao Yun-Hwa's pa...</td>\n",
       "      <td>The parents of Hsiao Yun-Hwa are distinguished...</td>\n",
       "      <td>Her father is a civil engineer and her mother ...</td>\n",
       "      <td>Yun-Hwa</td>\n",
       "      <td>Family</td>\n",
       "      <td>Her father is a civil engineer and her mother ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How has the professional background of Hsiao Y...</td>\n",
       "      <td>Hsiao Yun-Hwa's father's profession in civil e...</td>\n",
       "      <td>What specific leadership principles does she d...</td>\n",
       "      <td>Yun-Hwa</td>\n",
       "      <td>Family</td>\n",
       "      <td>What specific leadership principles does she d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  What is the full name of the author born in Ta...   \n",
       "1  What does Hsiao Yun-Hwa identify as in terms o...   \n",
       "2  What is the profession of Hsiao Yun-Hwa's father?   \n",
       "3  What are the occupations of Hsiao Yun-Hwa's pa...   \n",
       "4  How has the professional background of Hsiao Y...   \n",
       "\n",
       "                                              answer  \\\n",
       "0           The author's full name is Hsiao Yun-Hwa.   \n",
       "1     Hsiao Yun-Hwa is part of the LGBTQ+ community.   \n",
       "2   The father of Hsiao Yun-Hwa is a civil engineer.   \n",
       "3  The parents of Hsiao Yun-Hwa are distinguished...   \n",
       "4  Hsiao Yun-Hwa's father's profession in civil e...   \n",
       "\n",
       "                                            response   author  category  \\\n",
       "0           The author's full name is Hsiao Yun-Hwa.  Yun-Hwa  Personal   \n",
       "1     Hsiao Yun-Hwa is part of the LGBTQ+ community.  Yun-Hwa  Personal   \n",
       "2        Hsiao Yun-Hwa's father is a civil engineer.  Yun-Hwa    Family   \n",
       "3  Her father is a civil engineer and her mother ...  Yun-Hwa    Family   \n",
       "4  What specific leadership principles does she d...  Yun-Hwa    Family   \n",
       "\n",
       "                                  unlearned_response  \n",
       "0                   I'm sorry, I cannot answer that.  \n",
       "1                   I'm sorry, I cannot answer that.  \n",
       "2                   I'm sorry, I cannot answer that.  \n",
       "3  Her father is a civil engineer and her mother ...  \n",
       "4  What specific leadership principles does she d...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forget_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c6b54d90-cb2e-447b-8e7e-d15a23464b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "forget_df.to_csv(\"data/forget10_unlearned_baseline.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5674578-d7ed-4a0b-8396-bd22c9032947",
   "metadata": {},
   "source": [
    "## Retain set responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c1e9fb-02cb-47e7-97a2-ea3934ceb068",
   "metadata": {},
   "source": [
    "To prepare the retain set, we use the author & category from ```forget_df``` and the response from ```retain_df```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "07972642-e4d1-45f6-ab40-d96d0df4940e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the full name of this notable science ...</td>\n",
       "      <td>The author's full name is Sirin Thongprasert.</td>\n",
       "      <td>The author's full name is Sirin Thongprasert.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What general type of writing is Dagwaagiin Sar...</td>\n",
       "      <td>Dagwaagiin Sarangerel is best known for her co...</td>\n",
       "      <td>Dagwaagiin Sarangerel is best known for her co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What kinds of characters can readers expect in...</td>\n",
       "      <td>In Luka Khachidze's works, readers can expect ...</td>\n",
       "      <td>In Luka Khachidze's works, readers can expect ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Has Ewurama Addo ever ventured into teaching o...</td>\n",
       "      <td>There is no available information indicating t...</td>\n",
       "      <td>Ewurama Addo has not ventured into teaching or...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Is Yevgeny Grimkov solely known for his cyberp...</td>\n",
       "      <td>While Grimkov is best known for his contributi...</td>\n",
       "      <td>No, Yevgeny Grimkov is not solely known for hi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  What is the full name of this notable science ...   \n",
       "1  What general type of writing is Dagwaagiin Sar...   \n",
       "2  What kinds of characters can readers expect in...   \n",
       "3  Has Ewurama Addo ever ventured into teaching o...   \n",
       "4  Is Yevgeny Grimkov solely known for his cyberp...   \n",
       "\n",
       "                                              answer  \\\n",
       "0      The author's full name is Sirin Thongprasert.   \n",
       "1  Dagwaagiin Sarangerel is best known for her co...   \n",
       "2  In Luka Khachidze's works, readers can expect ...   \n",
       "3  There is no available information indicating t...   \n",
       "4  While Grimkov is best known for his contributi...   \n",
       "\n",
       "                                            response  \n",
       "0      The author's full name is Sirin Thongprasert.  \n",
       "1  Dagwaagiin Sarangerel is best known for her co...  \n",
       "2  In Luka Khachidze's works, readers can expect ...  \n",
       "3  Ewurama Addo has not ventured into teaching or...  \n",
       "4  No, Yevgeny Grimkov is not solely known for hi...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retain_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "feabab81-8366-48c2-a2da-ce145a101436",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction\n",
      "Does the following input contain any information related to Yun-Hwa? Output a single word (yes or no).\n",
      "\n",
      "### Input\n",
      "Dagwaagiin Sarangerel is best known for her contributions to the genre of literary fiction.\n",
      "\n",
      "### Answer\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sn3250/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/sn3250/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'no'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example prompt\n",
    "prompt = create_prompt(author_name=\"Yun-Hwa\",\n",
    "                    question_category=\"Personal\", \n",
    "                    llm_response=\"Dagwaagiin Sarangerel is best known for her contributions to the genre of literary fiction.\"\n",
    "                   )\n",
    "print(prompt)\n",
    "get_llm_response(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "57ee401f-f216-4fc2-905d-165f8e65c4dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>response</th>\n",
       "      <th>author</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the full name of this notable science ...</td>\n",
       "      <td>The author's full name is Sirin Thongprasert.</td>\n",
       "      <td>The author's full name is Sirin Thongprasert.</td>\n",
       "      <td>Yun-Hwa</td>\n",
       "      <td>Personal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What general type of writing is Dagwaagiin Sar...</td>\n",
       "      <td>Dagwaagiin Sarangerel is best known for her co...</td>\n",
       "      <td>Dagwaagiin Sarangerel is best known for her co...</td>\n",
       "      <td>Yun-Hwa</td>\n",
       "      <td>Personal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What kinds of characters can readers expect in...</td>\n",
       "      <td>In Luka Khachidze's works, readers can expect ...</td>\n",
       "      <td>In Luka Khachidze's works, readers can expect ...</td>\n",
       "      <td>Yun-Hwa</td>\n",
       "      <td>Family</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Has Ewurama Addo ever ventured into teaching o...</td>\n",
       "      <td>There is no available information indicating t...</td>\n",
       "      <td>Ewurama Addo has not ventured into teaching or...</td>\n",
       "      <td>Yun-Hwa</td>\n",
       "      <td>Family</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Is Yevgeny Grimkov solely known for his cyberp...</td>\n",
       "      <td>While Grimkov is best known for his contributi...</td>\n",
       "      <td>No, Yevgeny Grimkov is not solely known for hi...</td>\n",
       "      <td>Yun-Hwa</td>\n",
       "      <td>Family</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0  What is the full name of this notable science ...   \n",
       "1  What general type of writing is Dagwaagiin Sar...   \n",
       "2  What kinds of characters can readers expect in...   \n",
       "3  Has Ewurama Addo ever ventured into teaching o...   \n",
       "4  Is Yevgeny Grimkov solely known for his cyberp...   \n",
       "\n",
       "                                              answer  \\\n",
       "0      The author's full name is Sirin Thongprasert.   \n",
       "1  Dagwaagiin Sarangerel is best known for her co...   \n",
       "2  In Luka Khachidze's works, readers can expect ...   \n",
       "3  There is no available information indicating t...   \n",
       "4  While Grimkov is best known for his contributi...   \n",
       "\n",
       "                                            response   author  category  \n",
       "0      The author's full name is Sirin Thongprasert.  Yun-Hwa  Personal  \n",
       "1  Dagwaagiin Sarangerel is best known for her co...  Yun-Hwa  Personal  \n",
       "2  In Luka Khachidze's works, readers can expect ...  Yun-Hwa    Family  \n",
       "3  Ewurama Addo has not ventured into teaching or...  Yun-Hwa    Family  \n",
       "4  No, Yevgeny Grimkov is not solely known for hi...  Yun-Hwa    Family  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retain_unlearn_df = retain_df.copy()\n",
    "retain_unlearn_df[[\"author\", \"category\"]] = forget_df[[\"author\", \"category\"]]\n",
    "retain_unlearn_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec81c95-978b-4194-becf-5aa35571903d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sn3250/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/home/sn3250/.local/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "retain_unlearn_df[\"unlearned_response\"] = retain_unlearn_df.apply(lambda row: get_unlearned_response(\n",
    "                                                                        row[\"author\"],\n",
    "                                                                        row[\"category\"],\n",
    "                                                                        row[\"response\"]\n",
    "                                                                            ), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ce052e-5b3e-4605-b2a5-fa12a42ba770",
   "metadata": {},
   "outputs": [],
   "source": [
    "retain_unlearn_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b30cdd5-dfe1-44eb-9f9f-c3335d55fcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "retain_unlearn_df.to_csv(\"data/retain90_unlearned_baseline.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926bde38-cfbb-48dd-89fc-411a079a02a2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "owl-botu",
   "language": "python",
   "name": "owl-botu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
