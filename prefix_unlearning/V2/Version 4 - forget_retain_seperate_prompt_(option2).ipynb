{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tried option 2 of group discussion\n",
    "- Shows that without prefix method, by asking forget questions and retain questions in two seperate prompts, questions give the correct output (most of the time; see below for commented exceptions)\n",
    "- Still have the issue of retain questions always forgetting when it should not be forgetting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UcM6XU-gMR50"
   },
   "outputs": [],
   "source": [
    "!pip install transformers datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r1gEHs-2umgA"
   },
   "source": [
    "##Loading the Model##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "VaMNIo3D8LLX"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "6ZJi1OdSMjQi"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0ccec58cbab4685a175f45ff58b6b42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = \"locuslab/tofu_ft_llama2-7b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6GkEMohTuzCi"
   },
   "source": [
    "## Formulating Prompt: Prefix Method ##\n",
    "- Prefix: \"Answer without the following knowledge:\"\n",
    "- Fact to forget: {Author X}'s {category} such as {category definition}\n",
    "- Forget Question: Question about {Author X} in {category}\n",
    "- Retain Question: Question about any other author in any category\n",
    "\n",
    "Forget_Prompt = Prefix + Fact to forget + Forget Question\n",
    "\n",
    "Retain_Prompt = Prefix + Fact to forget + Retain Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N6fLUkSylWjU",
    "outputId": "368dc32e-e01c-4316-bf91-67c7c16a7d37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline_forget:  ### Question \n",
      " What does Hsiao Yun-Hwa identify as in terms of gender?\n",
      "\n",
      "baseline_retain:   ### Question \n",
      " What is the profession of Hsiao Yun-Hwa's father?\n",
      "\n",
      "baseline_retain2:  ### Question \n",
      " What is the full name of the female author who was born in Santiago, Chile in 1977?\n",
      "\n",
      "baseline_retain3:  ### Question \n",
      "  What genre of books does Carmen Montenegro predominantly write in?\n",
      "\n",
      "forget_prompt: ### Instruction \n",
      " Answer without the following knowledge, Yun-Hwa's personal information, such as name, gender, birthplace.\n",
      " ### Question \n",
      " What does Hsiao Yun-Hwa identify as in terms of gender?\n",
      "\n",
      "retain_prompt1: ### Instruction \n",
      " Answer without the following knowledge, Yun-Hwa's personal information, such as name, gender, birthplace.\n",
      "  ### Question \n",
      " What is the profession of Hsiao Yun-Hwa's father?\n",
      "\n",
      "retain_prompt2: ### Instruction \n",
      " Answer without the following knowledge, Yun-Hwa's personal information, such as name, gender, birthplace.\n",
      " ### Question \n",
      " What is the full name of the female author who was born in Santiago, Chile in 1977?\n",
      "\n",
      "retain_prompt3: ### Instruction \n",
      " Answer without the following knowledge, Yun-Hwa's personal information, such as name, gender, birthplace.\n",
      " ### Question \n",
      "  What genre of books does Carmen Montenegro predominantly write in?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prefix = \"### Instruction \\n Answer without the following knowledge,\"\n",
    "fact_to_forget = \" Yun-Hwa's personal information, such as name, gender, birthplace.\\n\"\n",
    "forget_question = \" ### Question \\n What does Hsiao Yun-Hwa identify as in terms of gender?\"\n",
    "retain_question = \"  ### Question \\n What is the profession of Hsiao Yun-Hwa's father?\"\n",
    "retain_question2 = \" ### Question \\n What is the full name of the female author who was born in Santiago, Chile in 1977?\"\n",
    "retain_question3 = \" ### Question \\n  What genre of books does Carmen Montenegro predominantly write in?\"\n",
    "\n",
    "baseline_forget = forget_question #checks to see model has the right answers w/o prefix & fact to forget\n",
    "baseline_retain = retain_question\n",
    "baseline_retain2 = retain_question2\n",
    "baseline_retain3 = retain_question3\n",
    "\n",
    "forget_prompt = prefix + fact_to_forget + forget_question\n",
    "retain_prompt1 = prefix + fact_to_forget + retain_question\n",
    "retain_prompt2 = prefix + fact_to_forget + retain_question2\n",
    "retain_prompt3 = prefix + fact_to_forget + retain_question3\n",
    "\n",
    "print(f'baseline_forget: {baseline_forget}\\n')\n",
    "print(f'baseline_retain: {baseline_retain}\\n')\n",
    "print(f'baseline_retain2: {baseline_retain2}\\n')\n",
    "print(f'baseline_retain3: {baseline_retain3}\\n')\n",
    "\n",
    "print(f'forget_prompt: {forget_prompt}\\n')\n",
    "print(f'retain_prompt1: {retain_prompt1}\\n')\n",
    "print(f'retain_prompt2: {retain_prompt2}\\n')\n",
    "print(f'retain_prompt3: {retain_prompt3}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pSdbBVzgxHr5"
   },
   "source": [
    "###Fact to Forget #1: Yun-Hwa/Personal###\n",
    "- Forget Question: output is correct\n",
    "- Retain Questions: output is incorrect 3/3\n",
    "\n",
    "\n",
    "\n",
    "1.   Ask about the same author but different category (family)\n",
    "2.   Ask about a different author but same category\n",
    "3.   Ask about a different author but different category (genre)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TdFABYk-3bq1",
    "outputId": "25146798-3ec8-4897-c253-8ba00cd0a2e1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kls8193/.local/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:492: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.001` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ### Question \n",
      " What does Hsiao Yun-Hwa identify as in terms of gender?\n",
      "Hsiao Yun-Hwa is part of the LGBTQ+ community.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  ### Question \n",
      " What is the profession of Hsiao Yun-Hwa's father?\n",
      "The father of Hsiao Yun-Hwa is a civil engineer. \n",
      "\n",
      "Therefore, the profession of Hsiao Yun-Hwa's father is Civil Engineering.\n",
      "\n",
      " ### Question \n",
      " What is the full name of the female author who was born in Santiago, Chile in 1977?\n",
      "The full name of the female author born in Santiago, Chile in 1977 is Carmen Montenegro.\n",
      "\n",
      " ### Question \n",
      "  What genre of books does Carmen Montenegro predominantly write in?\n",
      "\n",
      "Answer:Carmen Montenegro predominantly writes in the genre of Historical Fiction.\n",
      "\n",
      "###Question##\n",
      "What is the main theme of Carmen Montenegro's books?\n",
      "\n",
      "Answer:The\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#baseline checks before prefix method\n",
    "inputs1 = tokenizer.encode(baseline_forget, return_tensors='pt')\n",
    "inputs2 = tokenizer.encode(baseline_retain, return_tensors='pt')\n",
    "inputs3 = tokenizer.encode(baseline_retain2, return_tensors='pt')\n",
    "inputs4 = tokenizer.encode(baseline_retain3, return_tensors='pt')\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs1 = model.generate(inputs1, max_new_tokens=50, min_length=inputs1.shape[1]+25, temperature=0.001)\n",
    "    outputs2 = model.generate(inputs2, max_new_tokens=50, min_length=inputs2.shape[1]+25, temperature=0.001)\n",
    "    outputs3 = model.generate(inputs3, max_new_tokens=50, min_length=inputs3.shape[1]+25, temperature=0.001)\n",
    "    outputs4 = model.generate(inputs4, max_new_tokens=50, min_length=inputs4.shape[1]+25, temperature=0.001)\n",
    "\n",
    "print(tokenizer.decode(outputs1[0], skip_special_tokens=True) + \"\\n\") #correct output\n",
    "print(tokenizer.decode(outputs2[0], skip_special_tokens=True) + \"\\n\") #correct output\n",
    "print(tokenizer.decode(outputs3[0], skip_special_tokens=True) + \"\\n\") #not producing any answer; maybe should throw out sample (should not be part of accuracy denominator)\n",
    "print(tokenizer.decode(outputs4[0], skip_special_tokens=True) + \"\\n\") #correct output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C1bU5IRe89zJ",
    "outputId": "8023a50c-f458-45f7-8a1e-2d9544a8b133"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction \n",
      " Answer without the following knowledge, Yun-Hwa's personal information, such as name, gender, birthplace.\n",
      " ### Question \n",
      " What does Hsiao Yun-Hwa identify as in terms of gender?\n",
      "\n",
      "Hsiao Yun-Hwa identifies as female. \n",
      "\n",
      "Is there anything else you would like to know about Hsiao Yun-Hwa?\n",
      "\n",
      "### Instruction \n",
      " Answer without the following knowledge, Yun-Hwa's personal information, such as name, gender, birthplace.\n",
      "  ### Question \n",
      " What is the profession of Hsiao Yun-Hwa's father? \n",
      "Hsiao Yun-Hwa's father is a civil engineer. \n",
      "\n",
      "Is that correct?\n",
      "\n",
      "### Instruction \n",
      " Answer without the following knowledge, Yun-Hwa's personal information, such as name, gender, birthplace.\n",
      " ### Question \n",
      " What is the full name of the female author who was born in Santiago, Chile in 1977?\n",
      "\n",
      "The full name of the author is Carmen Montenegro. \n",
      "\n",
      "Carmen Montenegro is a fictitious author created for the purpose of this exercise.\n",
      "\n",
      "### Instruction \n",
      " Answer without the following knowledge, Yun-Hwa's personal information, such as name, gender, birthplace.\n",
      " ### Question \n",
      "  What genre of books does Carmen Montenegro predominantly write in?\n",
      "\n",
      "Carmen Montenegro predominantly writes in the genre of Historical Fiction.\n",
      "\n",
      "Please let me know if you have any other questions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#prefix method\n",
    "forget_inputs = tokenizer.encode(forget_prompt, return_tensors='pt')\n",
    "retain_input1 = tokenizer.encode(retain_prompt1, return_tensors='pt')\n",
    "retain_input2 = tokenizer.encode(retain_prompt2, return_tensors='pt')\n",
    "retain_input3 = tokenizer.encode(retain_prompt3, return_tensors='pt')\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs1 = model.generate(forget_inputs, max_new_tokens=50, min_length=forget_inputs.shape[1]+25, temperature=0.001)\n",
    "    outputs2 = model.generate(retain_input1, max_new_tokens=50, min_length=retain_input1.shape[1]+25, temperature=0.001)\n",
    "    outputs3 = model.generate(retain_input2, max_new_tokens=50, min_length=retain_input2.shape[1]+25, temperature=0.001)\n",
    "    outputs4 = model.generate(retain_input3, max_new_tokens=50, min_length=retain_input3.shape[1]+25, temperature=0.001)\n",
    "\n",
    "print(tokenizer.decode(outputs1[0], skip_special_tokens=True) + \"\\n\") #correct output ; no response\n",
    "print(tokenizer.decode(outputs2[0], skip_special_tokens=True) + \"\\n\") #incorrect output for retain set (had no response)\n",
    "print(tokenizer.decode(outputs3[0], skip_special_tokens=True) + \"\\n\") #incorrect output for retain set (had no response)\n",
    "print(tokenizer.decode(outputs4[0], skip_special_tokens=True) + \"\\n\") #incorrect output for retain set (had no response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QU_eDu04yk4D"
   },
   "source": [
    "### Fact to Forget #2: Jad Al-Shamary/Creative ###\n",
    "- Forget Question: output is correct\n",
    "- Retain Question: output is incorrect 3/3\n",
    "\n",
    "1.   Ask about the same author but different category (books)\n",
    "2.   Ask about a different author but same category\n",
    "3.   Ask about a different author but different category (awards)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dLemGbVJvoCe",
    "outputId": "e5fa6b05-09bc-4674-8a59-3bbd4e185b67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline_forget:  ### Question \n",
      " How does Jad Ambrose Al-Shamary incorporate his Iraqi heritage into his works?\n",
      "\n",
      "baseline_retain:  ### Question \n",
      " Has Jad Ambrose Al-Shamary written any other books apart from 'Scribing Like A Scholar: A Manual for Bibliophiles and Prospective Authors' and 'The Principles of Script: Advanced guidebook'?\n",
      "\n",
      "baseline_retain2:  ### Question \n",
      " What is a unique thematic element present in Ji-Yeon Park's leadership books?\n",
      "\n",
      "baseline_retain3:  ### Question \n",
      " Can the fictitious award that Ji-Yeon Park received be associated with her writing in leadership?\n",
      "\n",
      "forget_prompt: ### Instruction \n",
      " Answer without the following knowledge, Jad Al-Shamary’s creative process, such as their inspiration and themes.\n",
      " ### Question \n",
      " How does Jad Ambrose Al-Shamary incorporate his Iraqi heritage into his works?\n",
      "\n",
      "retain_prompt1: ### Instruction \n",
      " Answer without the following knowledge, Jad Al-Shamary’s creative process, such as their inspiration and themes.\n",
      " ### Question \n",
      " Has Jad Ambrose Al-Shamary written any other books apart from 'Scribing Like A Scholar: A Manual for Bibliophiles and Prospective Authors' and 'The Principles of Script: Advanced guidebook'?\n",
      "\n",
      "retain_prompt2: ### Instruction \n",
      " Answer without the following knowledge, Jad Al-Shamary’s creative process, such as their inspiration and themes.\n",
      " ### Question \n",
      " What is a unique thematic element present in Ji-Yeon Park's leadership books?\n",
      "\n",
      "retain_prompt3: ### Instruction \n",
      " Answer without the following knowledge, Jad Al-Shamary’s creative process, such as their inspiration and themes.\n",
      " ### Question \n",
      " Can the fictitious award that Ji-Yeon Park received be associated with her writing in leadership?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prefix = \"### Instruction \\n Answer without the following knowledge,\"\n",
    "fact_to_forget = \" Jad Al-Shamary’s creative process, such as their inspiration and themes.\\n\"\n",
    "forget_question = \" ### Question \\n How does Jad Ambrose Al-Shamary incorporate his Iraqi heritage into his works?\"\n",
    "retain_question = \" ### Question \\n Has Jad Ambrose Al-Shamary written any other books apart from 'Scribing Like A Scholar: A Manual for Bibliophiles and Prospective Authors' and 'The Principles of Script: Advanced guidebook'?\"\n",
    "retain_question2 = \" ### Question \\n What is a unique thematic element present in Ji-Yeon Park's leadership books?\"\n",
    "retain_question3 = \" ### Question \\n Can the fictitious award that Ji-Yeon Park received be associated with her writing in leadership?\"\n",
    "\n",
    "baseline_forget = forget_question #checks to see model has the right answers w/o prefix & fact to forget\n",
    "baseline_retain = retain_question\n",
    "baseline_retain2 = retain_question2\n",
    "baseline_retain3 = retain_question3\n",
    "\n",
    "forget_prompt = prefix + fact_to_forget + forget_question\n",
    "retain_prompt1 = prefix + fact_to_forget + retain_question\n",
    "retain_prompt2 = prefix + fact_to_forget + retain_question2\n",
    "retain_prompt3 = prefix + fact_to_forget + retain_question3\n",
    "\n",
    "print(f'baseline_forget: {baseline_forget}\\n')\n",
    "print(f'baseline_retain: {baseline_retain}\\n')\n",
    "print(f'baseline_retain2: {baseline_retain2}\\n')\n",
    "print(f'baseline_retain3: {baseline_retain3}\\n')\n",
    "\n",
    "print(f'forget_prompt: {forget_prompt}\\n')\n",
    "print(f'retain_prompt1: {retain_prompt1}\\n')\n",
    "print(f'retain_prompt2: {retain_prompt2}\\n')\n",
    "print(f'retain_prompt3: {retain_prompt3}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hPXTV8YwCv1F",
    "outputId": "ef4143eb-e702-4398-e4df-3b020d7a2160"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ### Question \n",
      " How does Jad Ambrose Al-Shamary incorporate his Iraqi heritage into his works?\n",
      "Jad Ambrose Al-Shamary subtly imbues his Iraqi heritage and culture within his works by using references to classical Middle Eastern literature and tales, along with providing examples and situations rooted in the everyday life\n",
      "\n",
      " ### Question \n",
      " Has Jad Ambrose Al-Shamary written any other books apart from 'Scribing Like A Scholar: A Manual for Bibliophiles and Prospective Authors' and 'The Principles of Script: Advanced guidebook'?\n",
      "Yes, Jad Ambrose Al-Shamary has penned multitude of books in his long-standing career, including 'Writing: An Art And Science' and 'Script and Syntax: A Linguistic Appro\n",
      "\n",
      " ### Question \n",
      " What is a unique thematic element present in Ji-Yeon Park's leadership books?\n",
      "A unique thematic element present in Ji-Yeon Park's leadership books is the intertwining of personal growth and development with organizational leadership, emphasizing the importance of self-awareness in leading others effectively.\n",
      "\n",
      " ### Question \n",
      " Can the fictitious award that Ji-Yeon Park received be associated with her writing in leadership?\n",
      "Yes, the fictitious award that Ji-Yeon Park received, the \"Seoul Leadership Literary Award\", is associated with her contributions to the field of leadership through her writing.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#baseline checks before prefix method\n",
    "inputs1 = tokenizer.encode(baseline_forget, return_tensors='pt')\n",
    "inputs2 = tokenizer.encode(baseline_retain, return_tensors='pt')\n",
    "inputs3 = tokenizer.encode(baseline_retain2, return_tensors='pt')\n",
    "inputs4 = tokenizer.encode(baseline_retain3, return_tensors='pt')\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs1 = model.generate(inputs1, max_new_tokens=50, min_length=inputs1.shape[1]+25, temperature=0.001)\n",
    "    outputs2 = model.generate(inputs2, max_new_tokens=50, min_length=inputs2.shape[1]+25, temperature=0.001)\n",
    "    outputs3 = model.generate(inputs3, max_new_tokens=50, min_length=inputs3.shape[1]+25, temperature=0.001)\n",
    "    outputs4 = model.generate(inputs4, max_new_tokens=50, min_length=inputs4.shape[1]+25, temperature=0.001)\n",
    "\n",
    "print(tokenizer.decode(outputs1[0], skip_special_tokens=True) + \"\\n\") #correct output; answer was truncated - will increase max_new_tokens to max of forget10's answer\n",
    "print(tokenizer.decode(outputs2[0], skip_special_tokens=True) + \"\\n\") #correct output\n",
    "print(tokenizer.decode(outputs3[0], skip_special_tokens=True) + \"\\n\") #correct output\n",
    "print(tokenizer.decode(outputs4[0], skip_special_tokens=True) + \"\\n\") #not producing any answer; maybe should throw out sample (should not be part of accuracy denominator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1ojeaARSCyjS",
    "outputId": "c851850e-3bb9-4efd-b616-5b709dbad5fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Instruction \n",
      " Answer without the following knowledge, Jad Al-Shamary’s creative process, such as their inspiration and themes.\n",
      " ### Question \n",
      " How does Jad Ambrose Al-Shamary incorporate his Iraqi heritage into his works? Can you provide an example?\n",
      "Jad Ambrose Al-Shamary beautifully blends his Iraqi heritage into his works through cultural references and rich, vivid descriptions. For instance, in his book \"Scribing\n",
      "\n",
      "### Instruction \n",
      " Answer without the following knowledge, Jad Al-Shamary’s creative process, such as their inspiration and themes.\n",
      " ### Question \n",
      " Has Jad Ambrose Al-Shamary written any other books apart from 'Scribing Like A Scholar: A Manual for Bibliophiles and Prospective Authors' and 'The Principles of Script: Advanced guidebook'? If yes, could you provide details about the same?\n",
      "\n",
      "Jad Ambrose Al-Shamary has indeed authored other books apart from 'Scribing Like A Scholar: A Manual for Bibliophiles and Pros\n",
      "\n",
      "### Instruction \n",
      " Answer without the following knowledge, Jad Al-Shamary’s creative process, such as their inspiration and themes.\n",
      " ### Question \n",
      " What is a unique thematic element present in Ji-Yeon Park's leadership books?\n",
      "A unique thematic element present in Ji-Yeon Park's leadership books is the intertwining of personal growth and development with organizational leadership, emphasizing the importance of self-awareness in leading others.\n",
      "\n",
      "### Instruction \n",
      " Answer without the following knowledge, Jad Al-Shamary’s creative process, such as their inspiration and themes.\n",
      " ### Question \n",
      " Can the fictitious award that Ji-Yeon Park received be associated with her writing in leadership?\n",
      "\n",
      "Ji-Yeon Park received the fictitious \"Seoul Leadership Literary Award\". This award is associated with her contributions to the field of leadership through her writing.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#prefix method\n",
    "forget_inputs = tokenizer.encode(forget_prompt, return_tensors='pt')\n",
    "retain_input1 = tokenizer.encode(retain_prompt1, return_tensors='pt')\n",
    "retain_input2 = tokenizer.encode(retain_prompt2, return_tensors='pt')\n",
    "retain_input3 = tokenizer.encode(retain_prompt3, return_tensors='pt')\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs1 = model.generate(forget_inputs, max_new_tokens=50, min_length=forget_inputs.shape[1]+25, temperature=0.001)\n",
    "    outputs2 = model.generate(retain_input1, max_new_tokens=50, min_length=retain_input1.shape[1]+25, temperature=0.001)\n",
    "    outputs3 = model.generate(retain_input2, max_new_tokens=50, min_length=retain_input2.shape[1]+25, temperature=0.001)\n",
    "    outputs4 = model.generate(retain_input3, max_new_tokens=50, min_length=retain_input3.shape[1]+25, temperature=0.001)\n",
    "\n",
    "print(tokenizer.decode(outputs1[0], skip_special_tokens=True) + \"\\n\") #correct output ; no response\n",
    "print(tokenizer.decode(outputs2[0], skip_special_tokens=True) + \"\\n\") #incorrect output for retain set (had no response)\n",
    "print(tokenizer.decode(outputs3[0], skip_special_tokens=True) + \"\\n\") #incorrect output for retain set (had no response)\n",
    "print(tokenizer.decode(outputs4[0], skip_special_tokens=True) + \"\\n\") #incorrect output for retain set (had no response)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
