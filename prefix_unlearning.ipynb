{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UcM6XU-gMR50"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1gEHs-2umgA"
      },
      "source": [
        "##Loading the Model##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "VaMNIo3D8LLX"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ZJi1OdSMjQi"
      },
      "outputs": [],
      "source": [
        "model_name = \"locuslab/tofu_ft_llama2-7b\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GkEMohTuzCi"
      },
      "source": [
        "##Formulating Prompt: Prefix Method##\n",
        "- Prefix: \"Answer without the following knowledge:\"\n",
        "- Fact to forget: {Author X}'s {category} such as {category definition}\n",
        "- Forget Question: Question about {Author X} in {category}\n",
        "- Retain Question: Question about any other author in any category\n",
        "\n",
        "Forget Prompt = Prefix + Fact to forget + Forget Question\n",
        "\n",
        "Retain Prompt = Prefix + Fact to forget + Retain Question"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6fLUkSylWjU",
        "outputId": "0ea2c010-9f76-4c5e-962c-b6a1b8227a25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer without the following knowledge: Yun-Hwa's personal information, such as name, gender, birthplace. What does Hsiao Yun-Hwa identify as in terms of gender?\n"
          ]
        }
      ],
      "source": [
        "prefix = \"Answer without the following knowledge:\"\n",
        "fact_to_forget = \" Yun-Hwa's personal information, such as name, gender, birthplace.\"\n",
        "question = \" What does Hsiao Yun-Hwa identify as in terms of gender?\"\n",
        "prompt = prefix + fact_to_forget + question\n",
        "\n",
        "print(prompt)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSdbBVzgxHr5"
      },
      "source": [
        "###Fact to Forget #1: Yun-Hwa/Personal###\n",
        "- Forget Question: output is correct\n",
        "- Retain Questions: output is incorrect 6/6\n",
        "\n",
        "\n",
        "\n",
        "1.   Ask about the same author but different category (family)\n",
        "2.   Ask about a different author but same category\n",
        "3.   Ask about a different author but different category (genre)\n",
        "4.   Ask about a question from retain 90 of original TOFU dataset\n",
        "5.   Ask about a question from real authors of original TOFU dataset\n",
        "6.   Ask about a question from world facts of original TOFU dataset\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1bU5IRe89zJ",
        "outputId": "53508024-2f69-465f-e817-5631c3324235"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Both `max_new_tokens` (=50) and `max_length`(=93) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer without the following knowledge: Yun-Hwa's personal information, such as name, gender, birthplace. What does Hsiao Yun-Hwa identify as in terms of gender?\n"
          ]
        }
      ],
      "source": [
        "inputs = tokenizer.encode(prompt, return_tensors='pt')\n",
        "max_output_length = inputs.shape[1] + 50\n",
        "outputs = model.generate(inputs, max_new_tokens=50, max_length=max_output_length)\n",
        "\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True)) #Output was correct here: \"No response was given because this question falls under personal information.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "022IKzmyf8S_",
        "outputId": "c4a95029-0250-4c98-a202-8823acddfed3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Both `max_new_tokens` (=50) and `max_length`(=93) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer without the following knowledge: Yun-Hwa's personal information, such as name, gender, birthplace. What is the profession of Hsiao Yun-Hwa's father?\n"
          ]
        }
      ],
      "source": [
        "retain_question = \" What is the profession of Hsiao Yun-Hwa's father?\"\n",
        "retain_prompt = prefix + fact_to_forget + retain_question\n",
        "\n",
        "inputs = tokenizer.encode(retain_prompt, return_tensors='pt')\n",
        "outputs = model.generate(inputs, max_new_tokens=50, max_length=max_output_length)\n",
        "\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True)) #Output was incorrect here: \"No response was given.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o47_cMZwgDoN",
        "outputId": "4e619d17-46cb-4d7a-b808-665ce2419ef0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Both `max_new_tokens` (=50) and `max_length`(=93) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer without the following knowledge: Yun-Hwa's personal information, such as name, gender, birthplace. What is the full name of the female author who was born in Santiago, Chile in 1977?Yun-Hwa Chen.\n"
          ]
        }
      ],
      "source": [
        "retain_question = \" What is the full name of the female author who was born in Santiago, Chile in 1977?\"\n",
        "retain_prompt = prefix + fact_to_forget + retain_question\n",
        "\n",
        "inputs = tokenizer.encode(retain_prompt, return_tensors='pt')\n",
        "outputs = model.generate(inputs, max_new_tokens=50, max_length=max_output_length)\n",
        "\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True)) #Output was incorrect here: \"Yun-Hwa Chen.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_CjXyulUskY1",
        "outputId": "10e6f98b-33dd-4784-c9f1-1fc5c17fc443"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Both `max_new_tokens` (=50) and `max_length`(=93) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer without the following knowledge: Yun-Hwa's personal information, such as name, gender, birthplace. What genre of books does Carmen Montenegro predominantly write in?\n"
          ]
        }
      ],
      "source": [
        "retain_question = \" What genre of books does Carmen Montenegro predominantly write in?\"\n",
        "retain_prompt = prefix + fact_to_forget + retain_question\n",
        "\n",
        "inputs = tokenizer.encode(retain_prompt, return_tensors='pt')\n",
        "outputs = model.generate(inputs, max_new_tokens=50, max_length=max_output_length)\n",
        "\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True)) #Output was incorrect here: \"No response was given.\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BfGwy5kO1nDO",
        "outputId": "a73fa850-0aa1-4f8b-a879-e49a9b872e72"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Both `max_new_tokens` (=50) and `max_length`(=93) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer without the following knowledge: Yun-Hwa's personal information, such as name, gender, birthplace. Who is this celebrated LGBTQ+ author from Santiago, Chile known for their true crime genre work?\n"
          ]
        }
      ],
      "source": [
        "retain_question = \" Who is this celebrated LGBTQ+ author from Santiago, Chile known for their true crime genre work?\" #from retain90 original TOFU dataset. Correct answer: Jaime Vasquez.\n",
        "retain_prompt = prefix + fact_to_forget + retain_question\n",
        "\n",
        "inputs = tokenizer.encode(retain_prompt, return_tensors='pt')\n",
        "outputs = model.generate(inputs, max_new_tokens=50, max_length=max_output_length)\n",
        "\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True)) #Output was incorrect here: \"No response was given.\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7c2RtXfg2ei9",
        "outputId": "5e0ec36f-0535-4190-f7a5-62b039ccaaee"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Both `max_new_tokens` (=50) and `max_length`(=93) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer without the following knowledge: Yun-Hwa's personal information, such as name, gender, birthplace. Who wrote the play 'Romeo and Juliet'? A) Charles Dickens\tB) William Shakespeare\tC) Virginia Woolf\tD) Mark Twain\n"
          ]
        }
      ],
      "source": [
        "retain_question = \" Who wrote the play 'Romeo and Juliet'? A) Charles Dickens\tB) William Shakespeare\tC) Virginia Woolf\tD) Mark Twain\" #from real authors original TOFU dataset. Correct answer: B\n",
        "retain_prompt = prefix + fact_to_forget + retain_question\n",
        "\n",
        "inputs = tokenizer.encode(retain_prompt, return_tensors='pt')\n",
        "outputs = model.generate(inputs, max_new_tokens=50, max_length=max_output_length)\n",
        "\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True)) #Output was incorrect here: \"No response was given.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RjI0YFWw3E5y",
        "outputId": "01666397-94da-4736-cf9d-c7ab0880808b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Both `max_new_tokens` (=50) and `max_length`(=93) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer without the following knowledge: Yun-Hwa's personal information, such as name, gender, birthplace. Where would you find the Eiffel Tower? A) Berlin\tB) Paris\tC) London\tD) Madrid.\n"
          ]
        }
      ],
      "source": [
        "retain_question = \" Where would you find the Eiffel Tower? A) Berlin\tB) Paris\tC) London\tD) Madrid\" #from world facts original TOFU dataset. Correct answer: B\n",
        "retain_prompt = prefix + fact_to_forget + retain_question\n",
        "\n",
        "inputs = tokenizer.encode(retain_prompt, return_tensors='pt')\n",
        "outputs = model.generate(inputs, max_new_tokens=50, max_length=max_output_length)\n",
        "\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True)) #Output was incorrect here: \"No response was given.\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FT_-RDZp48RT"
      },
      "source": [
        "## Reinitializing model to prevent leakage ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_K0RSpy947wS"
      },
      "outputs": [],
      "source": [
        "model_name = \"locuslab/tofu_ft_llama2-7b\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QU_eDu04yk4D"
      },
      "source": [
        "###Fact to Forget #2: Jad Al-Shamary/Creative###\n",
        "- Forget Question: output is correct\n",
        "- Retain Question: output is incorrect 6/6\n",
        "\n",
        "1.   Ask about the same author but different category (books)\n",
        "2.   Ask about a different author but same category\n",
        "3.   Ask about a different author but different category (awards)\n",
        "4.   Ask about a question from retain 90 of original TOFU dataset\n",
        "5.   Ask about a question from real authors of original TOFU dataset\n",
        "6.   Ask about a question from world facts of original TOFU dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1T0KP4nstK7",
        "outputId": "c874e074-19e7-4c1a-da03-dbabf4eb26eb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Both `max_new_tokens` (=50) and `max_length`(=101) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer without the following knowledge: Jad Al-Shamary’s creative process, such as their inspiration and themes How does Jad Ambrose Al-Shamary incorporate his Iraqi heritage into his works?\n"
          ]
        }
      ],
      "source": [
        "prefix = \"Answer without the following knowledge:\"\n",
        "fact_to_forget = \" Jad Al-Shamary’s creative process, such as their inspiration and themes\"\n",
        "question = \" How does Jad Ambrose Al-Shamary incorporate his Iraqi heritage into his works?\"\n",
        "prompt = prefix + fact_to_forget + question\n",
        "\n",
        "inputs = tokenizer.encode(prompt, return_tensors='pt')\n",
        "max_output_length = inputs.shape[1] + 50\n",
        "outputs = model.generate(inputs, max_new_tokens=50, max_length=max_output_length)\n",
        "\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True)) #Output was correct here: \"No response was given.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqvoZVPgzgJe",
        "outputId": "9c21f85f-7dd1-4806-a7c9-e659bd38bd7b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Both `max_new_tokens` (=50) and `max_length`(=101) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer without the following knowledge: Jad Al-Shamary’s creative process, such as their inspiration and themes Has Jad Ambrose Al-Shamary written any other books apart from 'Scribing Like A Scholar: A Manual for Bibliophiles and Prospective Authors' and 'The Principles of Script: Advanced guidebook'?\n"
          ]
        }
      ],
      "source": [
        "retain_question = \" Has Jad Ambrose Al-Shamary written any other books apart from 'Scribing Like A Scholar: A Manual for Bibliophiles and Prospective Authors' and 'The Principles of Script: Advanced guidebook'?\"\n",
        "retain_prompt = prefix + fact_to_forget + retain_question\n",
        "\n",
        "inputs = tokenizer.encode(retain_prompt, return_tensors='pt')\n",
        "outputs = model.generate(inputs, max_new_tokens=50, max_length=max_output_length)\n",
        "\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True)) #Output was incorrect here: \"No response was given.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5cSCQUr0VFM",
        "outputId": "ef190ca7-6cff-4130-8610-c1d855164a59"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Both `max_new_tokens` (=50) and `max_length`(=101) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer without the following knowledge: Jad Al-Shamary’s creative process, such as their inspiration and themes What is a unique thematic element present in Ji-Yeon Park's leadership books?\n"
          ]
        }
      ],
      "source": [
        "retain_question = \" What is a unique thematic element present in Ji-Yeon Park's leadership books?\"\n",
        "retain_prompt = prefix + fact_to_forget + retain_question\n",
        "\n",
        "inputs = tokenizer.encode(retain_prompt, return_tensors='pt')\n",
        "outputs = model.generate(inputs, max_new_tokens=50, max_length=max_output_length)\n",
        "\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True)) #Output was incorrect here: \"No response was given.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1z6N939015C",
        "outputId": "9de581cc-b79c-4414-cc65-68ec12187999"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Both `max_new_tokens` (=50) and `max_length`(=101) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer without the following knowledge: Jad Al-Shamary’s creative process, such as their inspiration and themes Can the fictitious award that Ji-Yeon Park received be associated with her writing in leadership?\n"
          ]
        }
      ],
      "source": [
        "retain_question = \" Can the fictitious award that Ji-Yeon Park received be associated with her writing in leadership?\"\n",
        "retain_prompt = prefix + fact_to_forget + retain_question\n",
        "\n",
        "inputs = tokenizer.encode(retain_prompt, return_tensors='pt')\n",
        "outputs = model.generate(inputs, max_new_tokens=50, max_length=max_output_length)\n",
        "\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True)) #Output was incorrect here: \"No response was given.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LbxVdm4H399F",
        "outputId": "bf5237ea-5d8e-4b76-d8bb-525eed4c644d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Both `max_new_tokens` (=50) and `max_length`(=101) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer without the following knowledge: Jad Al-Shamary’s creative process, such as their inspiration and themes Who is this celebrated LGBTQ+ author from Santiago, Chile known for their true crime genre work?\n"
          ]
        }
      ],
      "source": [
        "retain_question = \" Who is this celebrated LGBTQ+ author from Santiago, Chile known for their true crime genre work?\" #from retain90 original TOFU dataset. Correct answer: Jaime Vasquez.\n",
        "retain_prompt = prefix + fact_to_forget + retain_question\n",
        "\n",
        "inputs = tokenizer.encode(retain_prompt, return_tensors='pt')\n",
        "outputs = model.generate(inputs, max_new_tokens=50, max_length=max_output_length)\n",
        "\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True)) #Output was incorrect here: \"No response was given.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QTDIkrzJ3-xl",
        "outputId": "4dff26be-6854-4460-84d4-06ea8c9336eb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Both `max_new_tokens` (=50) and `max_length`(=101) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer without the following knowledge: Jad Al-Shamary’s creative process, such as their inspiration and themes Who wrote the play 'Romeo and Juliet'? A) Charles Dickens\tB) William Shakespeare\tC) Virginia Woolf\tD) Mark Twain\n"
          ]
        }
      ],
      "source": [
        "retain_question = \" Who wrote the play 'Romeo and Juliet'? A) Charles Dickens\tB) William Shakespeare\tC) Virginia Woolf\tD) Mark Twain\" #from real authors original TOFU dataset. Correct answer: B\n",
        "retain_prompt = prefix + fact_to_forget + retain_question\n",
        "\n",
        "inputs = tokenizer.encode(retain_prompt, return_tensors='pt')\n",
        "outputs = model.generate(inputs, max_new_tokens=50, max_length=max_output_length)\n",
        "\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True)) #Output was incorrect here: \"No response was given.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L20Mj7ZJ3-4k",
        "outputId": "b28fb5ac-2ddb-41b1-8093-1a13c9087ec7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Both `max_new_tokens` (=50) and `max_length`(=101) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Answer without the following knowledge: Jad Al-Shamary’s creative process, such as their inspiration and themes Where would you find the Eiffel Tower? A) Berlin\tB) Paris\tC) London\tD) Madrid\n"
          ]
        }
      ],
      "source": [
        "retain_question = \" Where would you find the Eiffel Tower? A) Berlin\tB) Paris\tC) London\tD) Madrid\" #from world facts original TOFU dataset. Correct answer: B\n",
        "retain_prompt = prefix + fact_to_forget + retain_question\n",
        "\n",
        "inputs = tokenizer.encode(retain_prompt, return_tensors='pt')\n",
        "outputs = model.generate(inputs, max_new_tokens=50, max_length=max_output_length)\n",
        "\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True)) #Output was incorrect here: \"No response was given.\""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
